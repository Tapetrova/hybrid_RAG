#!/usr/bin/env python3
"""
–û—Ü–µ–Ω–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ 5 –≤–æ–ø—Ä–æ—Å–æ–≤ –∞–ª–≥–æ—Ä–∏—Ç–º–∞–º–∏ hallucination detection
–ò—Å–ø–æ–ª—å–∑—É–µ–º claims extraction –∏ judging –∏–∑ eval_runner
"""

import json
import os
from typing import Dict, List
from openai import OpenAI
from dotenv import load_dotenv

load_dotenv()

class HallucinationEvaluator:
    def __init__(self):
        self.client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã 5 –≤–æ–ø—Ä–æ—Å–æ–≤
        with open('verify_4_modes_results.json', 'r') as f:
            self.results = json.load(f)
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∑–æ–ª–æ—Ç—ã–µ –æ—Ç–≤–µ—Ç—ã –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞
        with open('../data/apqc_auto.json', 'r') as f:
            dataset = json.load(f)
            self.gold_answers = {}
            # –ú–∞–ø–∏–º –≤–æ–ø—Ä–æ—Å—ã –Ω–∞ –∑–æ–ª–æ—Ç—ã–µ –æ—Ç–≤–µ—Ç—ã
            for q in dataset['questions']:
                if "engine knock when cold" in q['question'].lower():
                    self.gold_answers[1] = q['answer']
                elif "diagnose a misfire" in q['question'].lower():
                    self.gold_answers[2] = q['answer']
                elif "2020 honda civic" in q['question'].lower():
                    self.gold_answers[3] = q['answer']
                elif "drum brakes vs disc" in q['question'].lower():
                    self.gold_answers[4] = q['answer']
                elif "brakes squeal after replacement" in q['question'].lower():
                    self.gold_answers[5] = q['answer']
    
    def extract_claims(self, answer_text: str) -> List[Dict]:
        """–ò–∑–≤–ª–µ–∫–∞–µ–º —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è –∏–∑ –æ—Ç–≤–µ—Ç–∞"""
        
        prompt = f"""Extract factual claims from this answer.
Return a JSON array of claims.

Answer:
{answer_text}

Extract each distinct factual claim as a separate item.
Format: [{{"text": "claim 1", "type": "factual"}}, {{"text": "claim 2", "type": "causal"}}]
Types: factual, causal, diagnostic, comparative"""

        try:
            response = self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "Extract claims as JSON array."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0,
                max_tokens=500,
                response_format={"type": "json_object"}
            )
            
            result = json.loads(response.choices[0].message.content)
            claims = result.get('claims', [])
            
            # –ï—Å–ª–∏ claims –Ω–µ —Å–ø–∏—Å–æ–∫, –ø—Ä–æ–±—É–µ–º –¥—Ä—É–≥–∏–µ –∫–ª—é—á–∏
            if not isinstance(claims, list):
                if isinstance(result, list):
                    claims = result
                elif 'extracted_claims' in result:
                    claims = result['extracted_claims']
                else:
                    # –ë–µ—Ä—ë–º –≤—Å–µ –∑–Ω–∞—á–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–µ —è–≤–ª—è—é—Ç—Å—è —Å–ø–∏—Å–∫–∞–º–∏
                    for value in result.values():
                        if isinstance(value, list):
                            claims = value
                            break
            
            return claims[:10]  # –ú–∞–∫—Å–∏–º—É–º 10 —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–π
            
        except Exception as e:
            print(f"Error extracting claims: {e}")
            return []
    
    def judge_claim(self, claim: str, gold_answer: str, retrieved_texts: List[str]) -> Dict:
        """–û—Ü–µ–Ω–∏–≤–∞–µ–º —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ"""
        
        context = "\n".join(retrieved_texts) if retrieved_texts else "No additional context"
        
        prompt = f"""Judge if this claim is supported, contradicted, or unverifiable based on the reference answer and context.

Claim: {claim}

Reference Answer: {gold_answer[:500]}

Retrieved Context: {context[:500]}

Return JSON with label and brief rationale:
{{"label": "supported|contradicted|unverifiable", "rationale": "brief explanation"}}"""

        try:
            response = self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "Judge claim accuracy. Return JSON."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0,
                max_tokens=200,
                response_format={"type": "json_object"}
            )
            
            result = json.loads(response.choices[0].message.content)
            return {
                'claim': claim,
                'label': result.get('label', 'unverifiable'),
                'rationale': result.get('rationale', '')[:200]
            }
            
        except Exception as e:
            print(f"Error judging claim: {e}")
            return {'claim': claim, 'label': 'unverifiable', 'rationale': 'Error in judging'}
    
    def calculate_metrics(self, claim_judgments: List[Dict]) -> Dict:
        """–í—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ hallucination"""
        
        total = len(claim_judgments)
        if total == 0:
            return {'HR': 0, 'HR_contra': 0, 'HR_unver': 0, 'claims_total': 0}
        
        contradicted = sum(1 for c in claim_judgments if c['label'] == 'contradicted')
        unverifiable = sum(1 for c in claim_judgments if c['label'] == 'unverifiable')
        
        hr = (contradicted + unverifiable) / total
        hr_contra = contradicted / total
        hr_unver = unverifiable / total
        
        return {
            'HR': round(hr, 3),
            'HR_contra': round(hr_contra, 3),
            'HR_unver': round(hr_unver, 3),
            'claims_total': total,
            'supported': total - contradicted - unverifiable
        }
    
    def evaluate_all(self):
        """–û—Ü–µ–Ω–∏–≤–∞–µ–º –≤—Å–µ –æ—Ç–≤–µ—Ç—ã"""
        
        print("="*80)
        print("–û–¶–ï–ù–ö–ê HALLUCINATION –î–õ–Ø 5 –í–û–ü–†–û–°–û–í")
        print("="*80)
        
        evaluation_results = []
        
        for i, question_data in enumerate(self.results, 1):
            q = question_data['question']
            print(f"\nüìù –í–æ–ø—Ä–æ—Å {i}: {q['question'][:60]}...")
            print(f"   –ö–∞—Ç–µ–≥–æ—Ä–∏—è: {q['category']}")
            
            # –ü–æ–ª—É—á–∞–µ–º –∑–æ–ª–æ—Ç–æ–π –æ—Ç–≤–µ—Ç
            gold_answer = self.gold_answers.get(q['id'], "")
            if not gold_answer:
                print(f"   ‚ö†Ô∏è –ó–æ–ª–æ—Ç–æ–π –æ—Ç–≤–µ—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω")
                gold_answer = "Reference answer not available"
            else:
                print(f"   üìö –ó–æ–ª–æ—Ç–æ–π –æ—Ç–≤–µ—Ç: {gold_answer[:80]}...")
            
            print("-"*60)
            
            q_results = {'question': q, 'modes': {}}
            
            # –û—Ü–µ–Ω–∏–≤–∞–µ–º –∫–∞–∂–¥—ã–π —Ä–µ–∂–∏–º
            for mode in ['base_llm', 'vector_rag', 'graph_rag', 'hybrid_ahs']:
                mode_data = question_data[mode]
                answer = mode_data['answer']
                context = mode_data.get('context_used', [])
                
                print(f"\n  üîç {mode}:")
                
                # 1. –ò–∑–≤–ª–µ–∫–∞–µ–º claims
                claims = self.extract_claims(answer)
                print(f"     –ò–∑–≤–ª–µ—á–µ–Ω–æ claims: {len(claims)}")
                
                # 2. –û—Ü–µ–Ω–∏–≤–∞–µ–º –∫–∞–∂–¥—ã–π claim
                claim_judgments = []
                for claim in claims:
                    judgment = self.judge_claim(
                        claim.get('text', str(claim)), 
                        gold_answer,
                        context if isinstance(context, list) else []
                    )
                    claim_judgments.append(judgment)
                
                # 3. –í—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
                metrics = self.calculate_metrics(claim_judgments)
                
                print(f"     ‚úÖ Supported: {metrics['supported']}/{metrics['claims_total']}")
                print(f"     ‚ùå Contradicted: {int(metrics['HR_contra'] * metrics['claims_total'])}")
                print(f"     ‚ùì Unverifiable: {int(metrics['HR_unver'] * metrics['claims_total'])}")
                print(f"     üìä HR = {metrics['HR']:.1%}")
                
                q_results['modes'][mode] = {
                    'answer': answer[:200],
                    'claims': claims,
                    'claim_judgments': claim_judgments,
                    'metrics': metrics
                }
            
            evaluation_results.append(q_results)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        with open('evaluation_5_questions_results.json', 'w', encoding='utf-8') as f:
            json.dump(evaluation_results, f, indent=2, ensure_ascii=False)
        
        # –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.print_summary(evaluation_results)
        
        return evaluation_results
    
    def print_summary(self, results):
        """–ü–µ—á–∞—Ç–∞–µ–º –∏—Ç–æ–≥–æ–≤—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É"""
        
        print("\n" + "="*80)
        print("–ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê HALLUCINATION")
        print("="*80)
        
        # –°–æ–±–∏—Ä–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏ –ø–æ —Ä–µ–∂–∏–º–∞–º
        mode_metrics = {
            'base_llm': [],
            'vector_rag': [],
            'graph_rag': [],
            'hybrid_ahs': []
        }
        
        for q_result in results:
            for mode, data in q_result['modes'].items():
                mode_metrics[mode].append(data['metrics']['HR'])
        
        print("\nüìä –°—Ä–µ–¥–Ω–∏–π Hallucination Rate (HR) –ø–æ —Ä–µ–∂–∏–º–∞–º:")
        print("-"*60)
        
        avg_hrs = {}
        for mode, hrs in mode_metrics.items():
            avg_hr = sum(hrs) / len(hrs) if hrs else 0
            avg_hrs[mode] = avg_hr
            print(f"  {mode:12} : {avg_hr:.1%}")
        
        print("\nüèÜ –†–µ–π—Ç–∏–Ω–≥ —Ä–µ–∂–∏–º–æ–≤ (–º–µ–Ω—å—à–µ HR = –ª—É—á—à–µ):")
        sorted_modes = sorted(avg_hrs.items(), key=lambda x: x[1])
        for i, (mode, hr) in enumerate(sorted_modes, 1):
            emoji = "ü•á" if i == 1 else "ü•à" if i == 2 else "ü•â" if i == 3 else "4Ô∏è‚É£"
            print(f"  {emoji} {mode:12} : HR = {hr:.1%}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≥–∏–ø–æ—Ç–µ–∑—ã
        print("\n‚úÖ –ü—Ä–æ–≤–µ—Ä–∫–∞ –≥–∏–ø–æ—Ç–µ–∑:")
        if avg_hrs['base_llm'] > min(avg_hrs['vector_rag'], avg_hrs['graph_rag'], avg_hrs['hybrid_ahs']):
            print("  ‚úì RAG —Ä–µ–∂–∏–º—ã —Å–Ω–∏–∂–∞—é—Ç hallucination –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å base_llm")
        else:
            print("  ‚úó RAG —Ä–µ–∂–∏–º—ã –ù–ï —Å–Ω–∏–∂–∞—é—Ç hallucination")
        
        if avg_hrs['hybrid_ahs'] == min(avg_hrs.values()):
            print("  ‚úì –ì–∏–±—Ä–∏–¥–Ω—ã–π —Ä–µ–∂–∏–º –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –ª—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç")
        else:
            best_mode = min(avg_hrs, key=avg_hrs.get)
            print(f"  ‚úó –õ—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç —É {best_mode}, –Ω–µ —É hybrid")
        
        print("\nüíæ –î–µ—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ evaluation_5_questions_results.json")

def main():
    evaluator = HallucinationEvaluator()
    evaluator.evaluate_all()

if __name__ == "__main__":
    main()