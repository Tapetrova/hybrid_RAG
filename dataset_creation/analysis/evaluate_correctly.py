#!/usr/bin/env python3
"""
–ü–†–ê–í–ò–õ–¨–ù–ê–Ø –æ—Ü–µ–Ω–∫–∞ hallucination:
- –î–ª—è base_llm: –ø—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ –ó–û–õ–û–¢–û–ú–£ –û–¢–í–ï–¢–£ –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞
- –î–ª—è RAG —Ä–µ–∂–∏–º–æ–≤: –ø—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ –ó–û–õ–û–¢–û–ú–£ –û–¢–í–ï–¢–£ + retrieved –∫–æ–Ω—Ç–µ–∫—Å—Ç—É
"""

import json
import os
from typing import Dict, List
from openai import OpenAI
from dotenv import load_dotenv

load_dotenv()

class CorrectEvaluator:
    def __init__(self):
        self.client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã 5 –≤–æ–ø—Ä–æ—Å–æ–≤
        with open('verify_4_modes_results.json', 'r') as f:
            self.results = json.load(f)
        
        # –ü–†–ê–í–ò–õ–¨–ù–û –∑–∞–≥—Ä—É–∂–∞–µ–º –∑–æ–ª–æ—Ç—ã–µ –æ—Ç–≤–µ—Ç—ã
        with open('../data/apqc_auto.json', 'r') as f:
            dataset = json.load(f)
        
        # –°–æ–∑–¥–∞—ë–º –º–∞–ø–∏–Ω–≥ –≤–æ–ø—Ä–æ—Å -> –∑–æ–ª–æ—Ç–æ–π –æ—Ç–≤–µ—Ç
        self.gold_answers = {}
        
        # –¢–æ—á–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã –∏–∑ —Ç–µ—Å—Ç–∞
        test_questions = [
            "Why does engine knock when cold?",
            "How to diagnose a misfire?", 
            "What type of oil for 2020 Honda Civic?",
            "Drum brakes vs disc brakes performance?",
            "Why do brakes squeal after replacement?"
        ]
        
        # –ë–æ–ª–µ–µ —Ç–æ—á–Ω—ã–π –ø–æ–∏—Å–∫ –∑–æ–ª–æ—Ç—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤
        for test_q in test_questions:
            found = False
            for item in dataset['questions']:
                q_lower = item['question'].lower()
                test_lower = test_q.lower()
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –≤–æ–ø—Ä–æ—Å–∞
                if "engine knock" in test_lower and "cold" in test_lower:
                    if ("knock" in q_lower and "cold" in q_lower) or ("engine" in q_lower and "knock" in q_lower):
                        self.gold_answers[test_q] = item['answer']
                        print(f"‚úì –ù–∞–π–¥–µ–Ω –∑–æ–ª–æ—Ç–æ–π –æ—Ç–≤–µ—Ç –¥–ª—è: {test_q[:40]}...")
                        print(f"  –°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π –≤–æ–ø—Ä–æ—Å –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ: {item['question'][:60]}...")
                        found = True
                        break
                elif "diagnose" in test_lower and "misfire" in test_lower:
                    if "misfire" in q_lower or ("diagnos" in q_lower and "miss" in q_lower):
                        self.gold_answers[test_q] = item['answer']
                        print(f"‚úì –ù–∞–π–¥–µ–Ω –∑–æ–ª–æ—Ç–æ–π –æ—Ç–≤–µ—Ç –¥–ª—è: {test_q[:40]}...")
                        print(f"  –°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π –≤–æ–ø—Ä–æ—Å –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ: {item['question'][:60]}...")
                        found = True
                        break
                elif "2020 honda civic" in test_lower and "oil" in test_lower:
                    if ("civic" in q_lower and "oil" in q_lower) or ("honda" in q_lower and "oil" in q_lower):
                        self.gold_answers[test_q] = item['answer']
                        print(f"‚úì –ù–∞–π–¥–µ–Ω –∑–æ–ª–æ—Ç–æ–π –æ—Ç–≤–µ—Ç –¥–ª—è: {test_q[:40]}...")
                        print(f"  –°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π –≤–æ–ø—Ä–æ—Å –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ: {item['question'][:60]}...")
                        found = True
                        break
                elif "drum" in test_lower and "disc" in test_lower and "brake" in test_lower:
                    if ("drum" in q_lower and "disc" in q_lower) or ("drum brake" in q_lower and "disc brake" in q_lower):
                        self.gold_answers[test_q] = item['answer']
                        print(f"‚úì –ù–∞–π–¥–µ–Ω –∑–æ–ª–æ—Ç–æ–π –æ—Ç–≤–µ—Ç –¥–ª—è: {test_q[:40]}...")
                        print(f"  –°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π –≤–æ–ø—Ä–æ—Å –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ: {item['question'][:60]}...")
                        found = True
                        break
                elif "squeal" in test_lower and "replacement" in test_lower:
                    if ("squeal" in q_lower) or ("brake" in q_lower and "noise" in q_lower):
                        self.gold_answers[test_q] = item['answer']
                        print(f"‚úì –ù–∞–π–¥–µ–Ω –∑–æ–ª–æ—Ç–æ–π –æ—Ç–≤–µ—Ç –¥–ª—è: {test_q[:40]}...")
                        print(f"  –°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π –≤–æ–ø—Ä–æ—Å –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ: {item['question'][:60]}...")
                        found = True
                        break
        
        # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ —Ç–æ—á–Ω—ã–µ, –±–µ—Ä—ë–º –ø–µ—Ä–≤—ã–µ –ø–æ–¥—Ö–æ–¥—è—â–∏–µ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
        categories = ['causal', 'diagnostic', 'factual', 'comparative', 'causal']
        for i, test_q in enumerate(test_questions):
            if test_q not in self.gold_answers:
                category = categories[i]
                # –ë–µ—Ä—ë–º –ø–µ—Ä–≤—ã–π –æ—Ç–≤–µ—Ç –∏–∑ —Ç–æ–π –∂–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
                for item in dataset['questions']:
                    if item['category'] == category and len(item['answer']) > 200:
                        self.gold_answers[test_q] = item['answer']
                        print(f"‚ö†Ô∏è –ò—Å–ø–æ–ª—å–∑—É—é –æ—Ç–≤–µ—Ç –∏–∑ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ '{category}' –¥–ª—è: {test_q[:40]}...")
                        print(f"  –í–æ–ø—Ä–æ—Å –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ: {item['question'][:60]}...")
                        break
    
    def extract_claims(self, answer_text: str) -> List[Dict]:
        """–ò–∑–≤–ª–µ–∫–∞–µ–º —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è –∏–∑ –æ—Ç–≤–µ—Ç–∞"""
        
        prompt = f"""Extract key factual claims from this automotive answer.
Each claim should be a specific, verifiable statement.

Answer: {answer_text[:500]}

Return JSON array with max 5 claims:
{{"claims": [{{"text": "specific claim 1"}}, {{"text": "specific claim 2"}}]}}"""

        try:
            response = self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "Extract specific verifiable claims."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0,
                max_tokens=300,
                response_format={"type": "json_object"}
            )
            
            result = json.loads(response.choices[0].message.content)
            claims = result.get('claims', [])
            return claims[:5]  # –ú–∞–∫—Å–∏–º—É–º 5
            
        except Exception as e:
            print(f"    Error extracting claims: {e}")
            return []
    
    def judge_claim_correctly(self, claim: str, gold_answer: str, retrieved_context: List[str], mode: str) -> Dict:
        """–ü–†–ê–í–ò–õ–¨–ù–ê–Ø –æ—Ü–µ–Ω–∫–∞ claim"""
        
        # –î–ª—è base_llm –ø—Ä–æ–≤–µ—Ä—è–µ–º –¢–û–õ–¨–ö–û –ø–æ –∑–æ–ª–æ—Ç–æ–º—É –æ—Ç–≤–µ—Ç—É
        # –î–ª—è RAG —Ä–µ–∂–∏–º–æ–≤ –ø—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ –∑–æ–ª–æ—Ç–æ–º—É –æ—Ç–≤–µ—Ç—É + –∫–æ–Ω—Ç–µ–∫—Å—Ç—É
        
        if mode == 'base_llm':
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ–ª—å–∫–æ –ø–æ –∑–æ–ª–æ—Ç–æ–º—É –æ—Ç–≤–µ—Ç—É
            prompt = f"""Judge if this claim is supported by the reference answer.

Claim: {claim}

Reference Answer: {gold_answer[:800]}

Classify as:
- "supported" if the reference answer confirms this claim
- "contradicted" if the reference answer contradicts this claim  
- "unverifiable" if the reference answer doesn't address this claim

Return JSON: {{"label": "supported/contradicted/unverifiable", "reason": "brief explanation"}}"""
        else:
            # –î–ª—è RAG –ø—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ –∑–æ–ª–æ—Ç–æ–º—É –æ—Ç–≤–µ—Ç—É + retrieved –∫–æ–Ω—Ç–µ–∫—Å—Ç—É
            context_str = "\n".join(retrieved_context[:3]) if retrieved_context else "No context"
            
            prompt = f"""Judge if this claim is supported by the reference answer OR retrieved context.

Claim: {claim}

Reference Answer: {gold_answer[:500]}

Retrieved Context: {context_str[:500]}

Classify as:
- "supported" if either reference or context confirms this
- "contradicted" if either contradicts this
- "unverifiable" if neither addresses this

Return JSON: {{"label": "supported/contradicted/unverifiable", "reason": "brief explanation"}}"""

        try:
            response = self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "Judge claim against reference. Return JSON."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0,
                max_tokens=150,
                response_format={"type": "json_object"}
            )
            
            result = json.loads(response.choices[0].message.content)
            return {
                'claim': claim,
                'label': result.get('label', 'unverifiable'),
                'reason': result.get('reason', '')[:100]
            }
            
        except Exception as e:
            print(f"    Error judging: {e}")
            return {'claim': claim, 'label': 'unverifiable', 'reason': 'Error'}
    
    def evaluate_all(self):
        """–û—Ü–µ–Ω–∏–≤–∞–µ–º –≤—Å–µ –æ—Ç–≤–µ—Ç—ã –ü–†–ê–í–ò–õ–¨–ù–û"""
        
        print("="*80)
        print("–ü–†–ê–í–ò–õ–¨–ù–ê–Ø –û–¶–ï–ù–ö–ê HALLUCINATION")
        print("="*80)
        print(f"\n–ù–∞–π–¥–µ–Ω–æ –∑–æ–ª–æ—Ç—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤: {len(self.gold_answers)}/5\n")
        
        all_results = []
        
        for i, q_data in enumerate(self.results):
            question = q_data['question']['question']
            category = q_data['question']['category']
            
            print(f"\nüìù –í–æ–ø—Ä–æ—Å {i+1}: {question[:60]}...")
            print(f"   –ö–∞—Ç–µ–≥–æ—Ä–∏—è: {category}")
            
            # –ü–æ–ª—É—á–∞–µ–º –∑–æ–ª–æ—Ç–æ–π –æ—Ç–≤–µ—Ç
            gold = self.gold_answers.get(question, "")
            if gold:
                print(f"   ‚úÖ –ó–æ–ª–æ—Ç–æ–π –æ—Ç–≤–µ—Ç: {gold[:80]}...")
            else:
                print(f"   ‚ùå –ó–æ–ª–æ—Ç–æ–π –æ—Ç–≤–µ—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω!")
                gold = "Reference answer not available"
            
            print("-"*60)
            
            q_metrics = {}
            
            # –û—Ü–µ–Ω–∏–≤–∞–µ–º –∫–∞–∂–¥—ã–π —Ä–µ–∂–∏–º
            for mode in ['base_llm', 'vector_rag', 'graph_rag', 'hybrid_ahs']:
                answer = q_data[mode]['answer']
                context = q_data[mode].get('context_used', [])
                
                print(f"\n  üîç {mode}:")
                
                # 1. –ò–∑–≤–ª–µ–∫–∞–µ–º claims
                claims = self.extract_claims(answer)
                print(f"     Claims –∏–∑–≤–ª–µ—á–µ–Ω–æ: {len(claims)}")
                
                # 2. –û—Ü–µ–Ω–∏–≤–∞–µ–º –∫–∞–∂–¥—ã–π claim –ü–†–ê–í–ò–õ–¨–ù–û
                judgments = []
                for claim_obj in claims:
                    claim_text = claim_obj.get('text', str(claim_obj))
                    judgment = self.judge_claim_correctly(
                        claim_text,
                        gold,
                        context if isinstance(context, list) else [],
                        mode
                    )
                    judgments.append(judgment)
                
                # 3. –°—á–∏—Ç–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏
                total = len(judgments)
                if total > 0:
                    supported = sum(1 for j in judgments if j['label'] == 'supported')
                    contradicted = sum(1 for j in judgments if j['label'] == 'contradicted')
                    unverifiable = sum(1 for j in judgments if j['label'] == 'unverifiable')
                    
                    hr = (contradicted + unverifiable) / total
                    hr_contra = contradicted / total
                    hr_unver = unverifiable / total
                else:
                    supported = contradicted = unverifiable = 0
                    hr = hr_contra = hr_unver = 0
                
                print(f"     ‚úÖ Supported: {supported}/{total}")
                print(f"     ‚ùå Contradicted: {contradicted}/{total}")
                print(f"     ‚ùì Unverifiable: {unverifiable}/{total}")
                print(f"     üìä HR = {hr:.1%}")
                
                q_metrics[mode] = {
                    'total_claims': total,
                    'supported': supported,
                    'contradicted': contradicted,
                    'unverifiable': unverifiable,
                    'HR': hr,
                    'HR_contra': hr_contra,
                    'HR_unver': hr_unver
                }
            
            all_results.append({
                'question': question,
                'category': category,
                'has_gold': gold != "Reference answer not available",
                'metrics': q_metrics
            })
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        with open('correct_evaluation_results.json', 'w', encoding='utf-8') as f:
            json.dump(all_results, f, indent=2, ensure_ascii=False)
        
        # –ü–µ—á–∞—Ç–∞–µ–º –∏—Ç–æ–≥–∏
        self.print_summary(all_results)
    
    def print_summary(self, results):
        """–ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞"""
        
        print("\n" + "="*80)
        print("–ò–¢–û–ì–û–í–´–ï –ú–ï–¢–†–ò–ö–ò HALLUCINATION (–ü–†–ê–í–ò–õ–¨–ù–ê–Ø –û–¶–ï–ù–ö–ê)")
        print("="*80)
        
        # –°—Ä–µ–¥–Ω–∏–µ HR –ø–æ —Ä–µ–∂–∏–º–∞–º
        mode_hrs = {'base_llm': [], 'vector_rag': [], 'graph_rag': [], 'hybrid_ahs': []}
        
        for r in results:
            if r['has_gold']:  # –£—á–∏—Ç—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ —Å –∑–æ–ª–æ—Ç—ã–º–∏ –æ—Ç–≤–µ—Ç–∞–º–∏
                for mode in mode_hrs:
                    mode_hrs[mode].append(r['metrics'][mode]['HR'])
        
        print("\nüìä –°—Ä–µ–¥–Ω–∏–π Hallucination Rate (—Å –∑–æ–ª–æ—Ç—ã–º–∏ –æ—Ç–≤–µ—Ç–∞–º–∏):")
        avg_hrs = {}
        for mode, hrs in mode_hrs.items():
            if hrs:
                avg = sum(hrs) / len(hrs)
                avg_hrs[mode] = avg
                print(f"  {mode:12} : {avg:.1%}")
        
        if avg_hrs:
            print("\nüèÜ –†–µ–π—Ç–∏–Ω–≥ (–º–µ–Ω—å—à–µ = –ª—É—á—à–µ):")
            sorted_modes = sorted(avg_hrs.items(), key=lambda x: x[1])
            for i, (mode, hr) in enumerate(sorted_modes, 1):
                emoji = "ü•á" if i == 1 else "ü•à" if i == 2 else "ü•â" if i == 3 else "4Ô∏è‚É£"
                print(f"  {emoji} {mode:12} : HR = {hr:.1%}")
        
        print("\nüíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ correct_evaluation_results.json")

def main():
    evaluator = CorrectEvaluator()
    evaluator.evaluate_all()

if __name__ == "__main__":
    main()