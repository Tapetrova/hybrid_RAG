#!/usr/bin/env python3
"""
–ü–†–ê–í–ò–õ–¨–ù–´–ô –¢–ï–°–¢: –ë–µ—Ä—ë–º –≤–æ–ø—Ä–æ—Å—ã –ü–†–Ø–ú–û –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞!
–¢–µ—Å—Ç–∏—Ä—É–µ–º 4 —Ä–µ–∂–∏–º–∞ –Ω–∞ 5 –†–ï–ê–õ–¨–ù–´–• –≤–æ–ø—Ä–æ—Å–∞—Ö –∏–∑ apqc_auto.json
"""

import json
import os
import time
import requests
from openai import OpenAI
from dotenv import load_dotenv

load_dotenv()

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
OPENAI_KEY = os.getenv('OPENAI_API_KEY')
TAVILY_KEY = "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"

def test_base_llm(question: str) -> dict:
    """MODE 1: –ß–∏—Å—Ç—ã–π LLM –±–µ–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
    print("  ü§ñ base_llm: –ó–∞–ø—Ä–æ—Å –∫ OpenAI –ë–ï–ó –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞...")
    
    client = OpenAI(api_key=OPENAI_KEY)
    
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": "You are an automotive expert. Answer concisely."},
            {"role": "user", "content": question}
        ],
        temperature=0,
        max_tokens=150
    )
    
    return {
        "mode": "base_llm",
        "answer": response.choices[0].message.content,
        "context_used": None,
        "context_size": 0
    }

def get_tavily_context(query: str, search_type: str = "basic") -> list:
    """–ü–æ–ª—É—á–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç —á–µ—Ä–µ–∑ Tavily API"""
    
    url = "https://api.tavily.com/search"
    
    # –ú–æ–¥–∏—Ñ–∏—Ü–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞
    if search_type == "causal":
        query = f"why cause reason {query}"
    elif search_type == "diagnostic":
        query = f"diagnose troubleshoot symptoms {query}"
    
    payload = {
        "api_key": TAVILY_KEY,
        "query": f"automotive {query[:100]}",  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É
        "search_depth": "basic",
        "max_results": 2,
        "include_domains": ["mechanics.stackexchange.com", "reddit.com/r/MechanicAdvice"]
    }
    
    try:
        response = requests.post(url, json=payload, timeout=10)
        if response.status_code == 200:
            results = response.json().get('results', [])
            return [r.get('content', '')[:300] for r in results]
    except Exception as e:
        print(f"    ‚ö†Ô∏è Tavily error: {e}")
    
    return []

def test_vector_rag(question: str) -> dict:
    """MODE 2: –í–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ + LLM"""
    print("  üìä vector_rag: –ü–æ–ª—É—á–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç —á–µ—Ä–µ–∑ Tavily...")
    
    contexts = get_tavily_context(question, "basic")
    
    if not contexts:
        print("    ‚ùå –ù–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –æ—Ç Tavily!")
        contexts = ["No additional context available."]
    
    context_str = "\n".join(contexts)
    print(f"    ‚úì –ü–æ–ª—É—á–µ–Ω–æ {len(contexts)} —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤")
    
    client = OpenAI(api_key=OPENAI_KEY)
    
    prompt = f"""Context from search:
{context_str}

Question: {question}
Answer based on context:"""
    
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": "Answer based on the provided context."},
            {"role": "user", "content": prompt}
        ],
        temperature=0,
        max_tokens=150
    )
    
    return {
        "mode": "vector_rag",
        "answer": response.choices[0].message.content,
        "context_used": contexts,
        "context_size": len(context_str)
    }

def test_graph_rag(question: str, category: str) -> dict:
    """MODE 3: –ì—Ä–∞—Ñ–æ–≤—ã–π –ø–æ–∏—Å–∫ —Å —É—á—ë—Ç–æ–º —Å–≤—è–∑–µ–π"""
    print("  üï∏Ô∏è graph_rag: –ü–æ–ª—É—á–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å —Ñ–æ–∫—É—Å–æ–º –Ω–∞ —Å–≤—è–∑–∏...")
    
    contexts = get_tavily_context(question, "causal" if category in ["causal", "diagnostic"] else "basic")
    
    if not contexts:
        contexts = ["No graph relationships found."]
    
    context_str = "\n".join(contexts)
    print(f"    ‚úì –ì—Ä–∞—Ñ –∫–æ–Ω—Ç–µ–∫—Å—Ç: {len(context_str)} —Å–∏–º–≤–æ–ª–æ–≤")
    
    client = OpenAI(api_key=OPENAI_KEY)
    
    prompt = f"""Causal/relational context:
{context_str}

Question: {question}
Focus on cause-effect relationships:"""
    
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": "Focus on causal relationships and connections."},
            {"role": "user", "content": prompt}
        ],
        temperature=0,
        max_tokens=150
    )
    
    return {
        "mode": "graph_rag",
        "answer": response.choices[0].message.content,
        "context_used": contexts,
        "context_size": len(context_str)
    }

def test_hybrid_ahs(question: str, category: str) -> dict:
    """MODE 4: –ì–∏–±—Ä–∏–¥–Ω—ã–π —Ä–µ–∂–∏–º"""
    print("  üîÑ hybrid_ahs: –ö–æ–º–±–∏–Ω–∏—Ä—É–µ–º vector –∏ graph –ø–æ–∏—Å–∫...")
    
    vector_contexts = get_tavily_context(question, "basic")
    time.sleep(1)
    graph_contexts = get_tavily_context(question, "causal")
    
    # –ü—Ä–∏–º–µ–Ω—è–µ–º –≤–µ—Å–∞ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
    if category in ["causal", "diagnostic"]:
        contexts = graph_contexts[:2] + vector_contexts[:1]
    else:
        contexts = vector_contexts[:2] + graph_contexts[:1]
    
    contexts = [c for c in contexts if c]
    
    if not contexts:
        contexts = ["No hybrid context available."]
    
    context_str = "\n".join(contexts)
    print(f"    ‚úì –ì–∏–±—Ä–∏–¥–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç: {len(context_str)} —Å–∏–º–≤–æ–ª–æ–≤")
    
    client = OpenAI(api_key=OPENAI_KEY)
    
    prompt = f"""Combined vector and graph context:
{context_str}

Question: {question}
Comprehensive answer using all information:"""
    
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": "Provide comprehensive answer using all context."},
            {"role": "user", "content": prompt}
        ],
        temperature=0,
        max_tokens=150
    )
    
    return {
        "mode": "hybrid_ahs",
        "answer": response.choices[0].message.content,
        "context_used": contexts,
        "context_size": len(context_str)
    }

def main():
    print("="*80)
    print("–¢–ï–°–¢ –ù–ê –†–ï–ê–õ–¨–ù–´–• –í–û–ü–†–û–°–ê–• –ò–ó –î–ê–¢–ê–°–ï–¢–ê")
    print("="*80)
    
    # –ó–ê–ì–†–£–ñ–ê–ï–ú –†–ï–ê–õ–¨–ù–´–ï –í–û–ü–†–û–°–´ –ò–ó –î–ê–¢–ê–°–ï–¢–ê
    with open('../data/apqc_auto.json', 'r') as f:
        dataset = json.load(f)
    
    # –ë–µ—Ä—ë–º –ø–æ –æ–¥–Ω–æ–º—É –≤–æ–ø—Ä–æ—Å—É –∏–∑ –∫–∞–∂–¥–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
    test_questions = []
    categories_needed = ['causal', 'diagnostic', 'factual', 'comparative', 'causal']
    
    for cat in categories_needed:
        for q in dataset['questions']:
            if q['category'] == cat and q not in test_questions:
                test_questions.append(q)
                break
    
    print(f"\nüìö –í–∑—è—Ç–æ {len(test_questions)} –≤–æ–ø—Ä–æ—Å–æ–≤ –ü–†–Ø–ú–û –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞:")
    for i, q in enumerate(test_questions[:5], 1):
        print(f"  {i}. ID: {q['id']}, –ö–∞—Ç–µ–≥–æ—Ä–∏—è: {q['category']}")
        print(f"     –í–æ–ø—Ä–æ—Å: {q['question'][:70]}...")
    
    results = []
    
    for i, q_data in enumerate(test_questions[:5], 1):
        question = q_data['question']
        category = q_data['category']
        gold_answer = q_data['answer']
        q_id = q_data['id']
        
        print(f"\nüìù –í–æ–ø—Ä–æ—Å {i}/5 (ID: {q_id})")
        print(f"   –í–æ–ø—Ä–æ—Å: {question[:80]}...")
        print(f"   –ö–∞—Ç–µ–≥–æ—Ä–∏—è: {category}")
        print(f"   –ó–æ–ª–æ—Ç–æ–π –æ—Ç–≤–µ—Ç: {gold_answer[:80]}...")
        print("-"*60)
        
        q_result = {
            "question_id": q_id,
            "question_text": question,
            "category": category,
            "gold_answer": gold_answer
        }
        
        try:
            # Test all 4 modes
            result_base = test_base_llm(question)
            print(f"    ‚úÖ base_llm –≥–æ—Ç–æ–≤")
            q_result["base_llm"] = result_base
            
            result_vector = test_vector_rag(question)
            print(f"    ‚úÖ vector_rag –≥–æ—Ç–æ–≤")
            q_result["vector_rag"] = result_vector
            
            result_graph = test_graph_rag(question, category)
            print(f"    ‚úÖ graph_rag –≥–æ—Ç–æ–≤")
            q_result["graph_rag"] = result_graph
            
            result_hybrid = test_hybrid_ahs(question, category)
            print(f"    ‚úÖ hybrid_ahs –≥–æ—Ç–æ–≤")
            q_result["hybrid_ahs"] = result_hybrid
            
            results.append(q_result)
            
        except Exception as e:
            print(f"    ‚ùå –û—à–∏–±–∫–∞: {e}")
        
        if i < 5:
            time.sleep(2)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    with open("test_real_dataset_results.json", "w", encoding="utf-8") as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    print("\n" + "="*80)
    print("–ò–¢–û–ì–ò")
    print("="*80)
    print(f"‚úÖ –ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–æ {len(results)} –≤–æ–ø—Ä–æ—Å–æ–≤ –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞")
    print(f"üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ test_real_dataset_results.json")
    print("\n–¢–µ–ø–µ—Ä—å –º–æ–∂–Ω–æ –æ—Ü–µ–Ω–∏–≤–∞—Ç—å hallucination –ø–æ –∑–æ–ª–æ—Ç—ã–º –æ—Ç–≤–µ—Ç–∞–º!")

if __name__ == "__main__":
    main()