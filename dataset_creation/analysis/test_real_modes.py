#!/usr/bin/env python3
"""
–†–ï–ê–õ–¨–ù–´–ô –¢–ï–°–¢ –í–°–ï–• 4 –†–ï–ñ–ò–ú–û–í - –ë–ï–ó –ú–û–ö–û–í!
–ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–∞—Å—Ç–æ—è—â–∏–µ API –∏ Knowledge Manager
"""

import json
import requests
import numpy as np
from pathlib import Path
from typing import Dict, List
import os
from dotenv import load_dotenv
import time

# –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è
load_dotenv()

class TestRealModes:
    def __init__(self):
        self.openai_key = os.getenv('OPENAI_API_KEY')
        self.tavily_key = "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"
        self.serper_key = "4e74a3600041c6fdc32d1a7920f3b32413936ab8"
        
        # URLs –¥–ª—è Knowledge Manager (–µ—Å–ª–∏ –∑–∞–ø—É—â–µ–Ω)
        self.km_vector_url = "http://localhost:8098/retrievement/vector_retrieve"
        self.km_graph_url = "http://localhost:8098/retrievement/graph_retrieve"
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º 5 —Ç–µ—Å—Ç–æ–≤—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤
        self.test_questions = self.load_test_questions()
        
        # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã
        self.results = {
            'base_llm': [],
            'vector_rag': [],
            'graph_rag': [],
            'hybrid_ahs': []
        }
    
    def load_test_questions(self) -> List[Dict]:
        """–ó–∞–≥—Ä—É–∂–∞–µ–º 5 —Ç–µ—Å—Ç–æ–≤—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤"""
        with open("../data/apqc_auto.json", 'r') as f:
            data = json.load(f)
        
        test_q = []
        categories = ['causal', 'diagnostic', 'factual', 'comparative', 'causal']
        
        for cat in categories:
            for q in data['questions']:
                if q['category'] == cat and q not in test_q:
                    test_q.append(q)
                    break
        
        print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(test_q)} —Ç–µ—Å—Ç–æ–≤—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤:")
        for i, q in enumerate(test_q, 1):
            print(f"  {i}. [{q['category']}] {q['question'][:60]}...")
        
        return test_q[:5]
    
    def get_real_tavily_retrieval(self, question: str) -> List[Dict]:
        """–†–ï–ê–õ–¨–ù–´–ô –ø–æ–∏—Å–∫ —á–µ—Ä–µ–∑ Tavily API"""
        
        print(f"    üîç –í—ã–∑–æ–≤ Tavily API...")
        
        url = "https://api.tavily.com/search"
        
        payload = {
            "api_key": self.tavily_key,
            "query": f"automotive {question[:100]}",  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É
            "search_depth": "basic",
            "max_results": 3,
            "include_answer": False,
            "include_domains": [
                "mechanics.stackexchange.com",
                "reddit.com/r/MechanicAdvice",
                "yourmechanic.com"
            ]
        }
        
        try:
            response = requests.post(url, json=payload, timeout=10)
            if response.status_code == 200:
                data = response.json()
                results = []
                for r in data.get('results', []):
                    results.append({
                        'text': r.get('content', '')[:500],
                        'url': r.get('url', ''),
                        'score': r.get('score', 0.5)
                    })
                print(f"    ‚úÖ –ü–æ–ª—É—á–µ–Ω–æ {len(results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ—Ç Tavily")
                return results
            else:
                print(f"    ‚ùå Tavily error: {response.status_code}")
                return []
        except Exception as e:
            print(f"    ‚ùå Tavily exception: {e}")
            return []
    
    def test_base_llm(self, question: str) -> Dict:
        """–¢–µ—Å—Ç 1: –ë–∞–∑–æ–≤—ã–π LLM –±–µ–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
        
        from openai import OpenAI
        client = OpenAI(api_key=self.openai_key)
        
        try:
            response = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "You are an automotive expert. Answer concisely."},
                    {"role": "user", "content": question}
                ],
                temperature=0.0,
                max_tokens=200
            )
            
            answer = response.choices[0].message.content
            
            return {
                'mode': 'base_llm',
                'answer': answer,
                'context_used': None,
                'context_length': 0,
                'api_calls': {'openai': 1}
            }
        except Exception as e:
            print(f"    ‚ùå Error in base_llm: {e}")
            return {'mode': 'base_llm', 'answer': f'Error: {e}', 'context_used': None, 'context_length': 0}
    
    def test_vector_rag(self, question: str) -> Dict:
        """–¢–µ—Å—Ç 2: –í–µ–∫—Ç–æ—Ä–Ω—ã–π RAG —Å –†–ï–ê–õ–¨–ù–´–ú retrieval"""
        
        # –ü–æ–ª—É—á–∞–µ–º –†–ï–ê–õ–¨–ù–´–ô –∫–æ–Ω—Ç–µ–∫—Å—Ç —á–µ—Ä–µ–∑ Tavily
        contexts = self.get_real_tavily_retrieval(question)
        
        if not contexts:
            print("    ‚ö†Ô∏è –ù–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –æ—Ç Tavily, –∏—Å–ø–æ–ª—å–∑—É–µ–º base_llm")
            return self.test_base_llm(question)
        
        context_str = "\n\n".join([f"Source {i+1}: {c['text']}" for i, c in enumerate(contexts)])
        
        from openai import OpenAI
        client = OpenAI(api_key=self.openai_key)
        
        try:
            prompt = f"""Use the following search results to answer the question:

{context_str}

Question: {question}

Answer based on the search results:"""
            
            response = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "Answer based on the provided search results."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.0,
                max_tokens=200
            )
            
            answer = response.choices[0].message.content
            
            return {
                'mode': 'vector_rag',
                'answer': answer,
                'context_used': contexts,
                'context_length': len(context_str),
                'api_calls': {'tavily': 1, 'openai': 1}
            }
        except Exception as e:
            print(f"    ‚ùå Error in vector_rag: {e}")
            return {'mode': 'vector_rag', 'answer': f'Error: {e}', 'context_used': contexts, 'context_length': len(context_str)}
    
    def test_graph_rag(self, question: str, category: str) -> Dict:
        """–¢–µ—Å—Ç 3: –ì—Ä–∞—Ñ–æ–≤—ã–π RAG —Å —É–≥–ª—É–±–ª—ë–Ω–Ω—ã–º –ø–æ–∏—Å–∫–æ–º"""
        
        # –î–ª—è graph –¥–µ–ª–∞–µ–º –±–æ–ª–µ–µ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–π –ø–æ–∏—Å–∫
        if category in ['causal', 'diagnostic']:
            # –î–æ–±–∞–≤–ª—è–µ–º —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã –¥–ª—è –≥—Ä–∞—Ñ–æ–≤—ã—Ö —Å–≤—è–∑–µ–π
            enhanced_query = f"why how cause effect relationship {question}"
        else:
            enhanced_query = question
        
        # –ü–æ–ª—É—á–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
        contexts = self.get_real_tavily_retrieval(enhanced_query)
        
        if not contexts:
            print("    ‚ö†Ô∏è –ù–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è graph_rag")
            return self.test_base_llm(question)
        
        context_str = "\n\n".join([f"Relationship {i+1}: {c['text']}" for i, c in enumerate(contexts)])
        
        from openai import OpenAI
        client = OpenAI(api_key=self.openai_key)
        
        try:
            prompt = f"""Use the following information about relationships and causes:

{context_str}

Question: {question}

Focus on causal relationships and connections between components:"""
            
            response = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "Answer focusing on causal relationships and connections."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.0,
                max_tokens=200
            )
            
            answer = response.choices[0].message.content
            
            return {
                'mode': 'graph_rag',
                'answer': answer,
                'context_used': contexts,
                'context_length': len(context_str),
                'api_calls': {'tavily': 1, 'openai': 1}
            }
        except Exception as e:
            print(f"    ‚ùå Error in graph_rag: {e}")
            return {'mode': 'graph_rag', 'answer': f'Error: {e}', 'context_used': contexts, 'context_length': len(context_str)}
    
    def test_hybrid_ahs(self, question: str, category: str) -> Dict:
        """–¢–µ—Å—Ç 4: –ì–∏–±—Ä–∏–¥–Ω—ã–π —Ä–µ–∂–∏–º - –∫–æ–º–±–∏–Ω–∏—Ä—É–µ–º —Ä–∞–∑–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏"""
        
        # –î–µ–ª–∞–µ–º –î–í–ê —Ä–∞–∑–Ω—ã—Ö –ø–æ–∏—Å–∫–∞
        print("    üîç –ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫ (2 –∑–∞–ø—Ä–æ—Å–∞)...")
        
        # 1. –§–∞–∫—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫
        factual_contexts = self.get_real_tavily_retrieval(question)
        time.sleep(1)  # –ü–∞—É–∑–∞ –º–µ–∂–¥—É API –≤—ã–∑–æ–≤–∞–º–∏
        
        # 2. –ö–∞—É–∑–∞–ª—å–Ω—ã–π/–¥–∏–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫
        causal_query = f"why how diagnose troubleshoot {question}"
        causal_contexts = self.get_real_tavily_retrieval(causal_query)
        
        # –ö–æ–º–±–∏–Ω–∏—Ä—É–µ–º —Å –≤–µ—Å–∞–º–∏
        weights = {
            'causal': {'causal': 0.7, 'factual': 0.3},
            'diagnostic': {'causal': 0.7, 'factual': 0.3},
            'factual': {'causal': 0.3, 'factual': 0.7},
            'comparative': {'causal': 0.4, 'factual': 0.6}
        }
        
        w = weights.get(category, {'causal': 0.5, 'factual': 0.5})
        
        # –û—Ç–±–∏—Ä–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç—ã –ø–æ –≤–µ—Å–∞–º
        if w['factual'] > w['causal']:
            contexts = factual_contexts[:2] + causal_contexts[:1]
        else:
            contexts = causal_contexts[:2] + factual_contexts[:1]
        
        context_str = "\n\n".join([f"Context {i+1}: {c['text']}" for i, c in enumerate(contexts) if c])
        
        from openai import OpenAI
        client = OpenAI(api_key=self.openai_key)
        
        try:
            prompt = f"""Use the following hybrid context (factual + relational):

{context_str}

Question: {question}

Provide a comprehensive answer using both factual information and relationships:"""
            
            response = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "Provide comprehensive answer using all context."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.0,
                max_tokens=200
            )
            
            answer = response.choices[0].message.content
            
            return {
                'mode': 'hybrid_ahs',
                'answer': answer,
                'context_used': contexts,
                'context_length': len(context_str),
                'api_calls': {'tavily': 2, 'openai': 1}
            }
        except Exception as e:
            print(f"    ‚ùå Error in hybrid_ahs: {e}")
            return {'mode': 'hybrid_ahs', 'answer': f'Error: {e}', 'context_used': contexts, 'context_length': 0}
    
    def run_all_tests(self):
        """–ó–∞–ø—É—Å–∫–∞–µ–º –≤—Å–µ 4 —Ä–µ–∂–∏–º–∞ –Ω–∞ 5 –≤–æ–ø—Ä–æ—Å–∞—Ö"""
        
        print("\n" + "="*80)
        print("–†–ï–ê–õ–¨–ù–û–ï –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –í–°–ï–• 4 –†–ï–ñ–ò–ú–û–í")
        print("="*80)
        
        for i, q in enumerate(self.test_questions, 1):
            print(f"\nüìù –í–æ–ø—Ä–æ—Å {i}/{len(self.test_questions)}: {q['question'][:80]}...")
            print(f"   –ö–∞—Ç–µ–≥–æ—Ä–∏—è: {q['category']}")
            print(f"   –ó–æ–ª–æ—Ç–æ–π –æ—Ç–≤–µ—Ç: {q['answer'][:80]}...")
            print("-"*80)
            
            # –¢–µ—Å—Ç 1: base_llm
            print("  ü§ñ Testing base_llm...")
            result_base = self.test_base_llm(q['question'])
            self.results['base_llm'].append(result_base)
            print(f"     ‚úì –û—Ç–≤–µ—Ç: {result_base['answer'][:100]}...")
            
            # –¢–µ—Å—Ç 2: vector_rag
            print("\n  üìä Testing vector_rag with REAL Tavily...")
            result_vector = self.test_vector_rag(q['question'])
            self.results['vector_rag'].append(result_vector)
            print(f"     ‚úì –ö–æ–Ω—Ç–µ–∫—Å—Ç: {result_vector['context_length']} —Å–∏–º–≤–æ–ª–æ–≤")
            print(f"     ‚úì –û—Ç–≤–µ—Ç: {result_vector['answer'][:100]}...")
            
            # –¢–µ—Å—Ç 3: graph_rag
            print("\n  üï∏Ô∏è Testing graph_rag with enhanced search...")
            result_graph = self.test_graph_rag(q['question'], q['category'])
            self.results['graph_rag'].append(result_graph)
            print(f"     ‚úì –ì—Ä–∞—Ñ –∫–æ–Ω—Ç–µ–∫—Å—Ç: {result_graph['context_length']} —Å–∏–º–≤–æ–ª–æ–≤")
            print(f"     ‚úì –û—Ç–≤–µ—Ç: {result_graph['answer'][:100]}...")
            
            # –¢–µ—Å—Ç 4: hybrid_ahs
            print("\n  üîÑ Testing hybrid_ahs with dual search...")
            result_hybrid = self.test_hybrid_ahs(q['question'], q['category'])
            self.results['hybrid_ahs'].append(result_hybrid)
            print(f"     ‚úì –ì–∏–±—Ä–∏–¥–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç: {result_hybrid['context_length']} —Å–∏–º–≤–æ–ª–æ–≤")
            print(f"     ‚úì –û—Ç–≤–µ—Ç: {result_hybrid['answer'][:100]}...")
            
            # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –≤–æ–ø—Ä–æ—Å–∞–º–∏
            if i < len(self.test_questions):
                print("\n  ‚è≥ –ü–∞—É–∑–∞ 2 —Å–µ–∫—É–Ω–¥—ã –ø–µ—Ä–µ–¥ —Å–ª–µ–¥—É—é—â–∏–º –≤–æ–ø—Ä–æ—Å–æ–º...")
                time.sleep(2)
    
    def verify_differences(self):
        """–ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–ª–∏—á–∏—è –º–µ–∂–¥—É —Ä–µ–∂–∏–º–∞–º–∏"""
        
        print("\n" + "="*80)
        print("–ê–ù–ê–õ–ò–ó –†–ï–ó–£–õ–¨–¢–ê–¢–û–í")
        print("="*80)
        
        # 1. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        print("\nüìä –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞:")
        for mode in self.results:
            contexts = [r.get('context_length', 0) for r in self.results[mode]]
            avg_context = np.mean(contexts) if contexts else 0
            print(f"   {mode}: {avg_context:.0f} —Å–∏–º–≤–æ–ª–æ–≤ –≤ —Å—Ä–µ–¥–Ω–µ–º")
        
        # 2. API –≤—ã–∑–æ–≤—ã
        print("\nüí∞ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ API:")
        total_api_calls = {'tavily': 0, 'openai': 0}
        for mode in self.results:
            for r in self.results[mode]:
                if 'api_calls' in r:
                    for api, count in r.get('api_calls', {}).items():
                        total_api_calls[api] = total_api_calls.get(api, 0) + count
        
        print(f"   Tavily API: {total_api_calls.get('tavily', 0)} –≤—ã–∑–æ–≤–æ–≤")
        print(f"   OpenAI API: {total_api_calls.get('openai', 0)} –≤—ã–∑–æ–≤–æ–≤")
        
        # 3. –ö–∞—á–µ—Å—Ç–≤–æ –æ—Ç–≤–µ—Ç–æ–≤
        print("\n‚úÖ –°—Ç–∞—Ç—É—Å –æ—Ç–≤–µ—Ç–æ–≤:")
        for mode in self.results:
            errors = sum(1 for r in self.results[mode] if 'Error' in r.get('answer', ''))
            success = len(self.results[mode]) - errors
            print(f"   {mode}: {success}/{len(self.results[mode])} —É—Å–ø–µ—à–Ω—ã—Ö")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        with open("test_real_modes_results.json", 'w', encoding='utf-8') as f:
            json.dump(self.results, f, indent=2, ensure_ascii=False)
        
        print("\n‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ test_real_modes_results.json")

def main():
    print("üöÄ –ó–∞–ø—É—Å–∫ –†–ï–ê–õ–¨–ù–û–ì–û —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è...")
    print("‚ö†Ô∏è –ë—É–¥—É—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω—ã –Ω–∞—Å—Ç–æ—è—â–∏–µ API (Tavily + OpenAI)")
    
    tester = TestRealModes()
    tester.run_all_tests()
    tester.verify_differences()
    
    print("\n" + "="*80)
    print("–ì–û–¢–û–í–û!")
    print("="*80)

if __name__ == "__main__":
    main()