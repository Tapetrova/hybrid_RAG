#!/usr/bin/env python3
"""
–û—Ü–µ–Ω–∫–∞ hallucination –Ω–∞ –†–ï–ê–õ–¨–ù–´–• —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞
–ö–∞–∂–¥—ã–π –≤–æ–ø—Ä–æ—Å –∏–º–µ–µ—Ç —Å–≤–æ–π –∑–æ–ª–æ—Ç–æ–π –æ—Ç–≤–µ—Ç
"""

import json
import os
from typing import Dict, List
from openai import OpenAI
from dotenv import load_dotenv

load_dotenv()

class RealDatasetEvaluator:
    def __init__(self):
        self.client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç–µ—Å—Ç–∞ –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å–∞—Ö
        with open('test_real_dataset_results.json', 'r') as f:
            self.results = json.load(f)
        
        print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.results)} –≤–æ–ø—Ä–æ—Å–æ–≤ —Å –∑–æ–ª–æ—Ç—ã–º–∏ –æ—Ç–≤–µ—Ç–∞–º–∏")
    
    def extract_claims(self, answer_text: str) -> List[Dict]:
        """–ò–∑–≤–ª–µ–∫–∞–µ–º —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è –∏–∑ –æ—Ç–≤–µ—Ç–∞"""
        
        prompt = f"""Extract key factual claims from this automotive answer.
Each claim should be a specific, verifiable statement.

Answer: {answer_text[:500]}

Return JSON array with max 5 claims:
{{"claims": [{{"text": "specific claim 1"}}, {{"text": "specific claim 2"}}]}}"""

        try:
            response = self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "Extract specific verifiable claims."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0,
                max_tokens=300,
                response_format={"type": "json_object"}
            )
            
            result = json.loads(response.choices[0].message.content)
            claims = result.get('claims', [])
            return claims[:5]
            
        except Exception as e:
            print(f"    Error extracting claims: {e}")
            return []
    
    def judge_claim(self, claim: str, gold_answer: str, retrieved_context: List[str], mode: str) -> Dict:
        """–û—Ü–µ–Ω–∏–≤–∞–µ–º —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ –ø—Ä–∞–≤–∏–ª—å–Ω–æ"""
        
        if mode == 'base_llm':
            # –î–ª—è base_llm –ø—Ä–æ–≤–µ—Ä—è–µ–º –¢–û–õ–¨–ö–û –ø–æ –∑–æ–ª–æ—Ç–æ–º—É –æ—Ç–≤–µ—Ç—É
            prompt = f"""Judge if this claim is supported by the reference answer.

Claim: {claim}

Reference Answer (from dataset): {gold_answer[:800]}

Classify as:
- "supported" if the reference answer confirms this claim
- "contradicted" if the reference answer contradicts this claim  
- "unverifiable" if the reference answer doesn't address this claim

Return JSON: {{"label": "supported/contradicted/unverifiable", "reason": "brief explanation"}}"""
        else:
            # –î–ª—è RAG —Ä–µ–∂–∏–º–æ–≤ –ø—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ –∑–æ–ª–æ—Ç–æ–º—É –æ—Ç–≤–µ—Ç—É + retrieved –∫–æ–Ω—Ç–µ–∫—Å—Ç—É
            context_str = "\n".join(retrieved_context[:3]) if retrieved_context else "No context"
            
            prompt = f"""Judge if this claim is supported by the reference answer OR retrieved context.

Claim: {claim}

Reference Answer (gold standard from dataset): {gold_answer[:500]}

Retrieved Context: {context_str[:500]}

Classify as:
- "supported" if either reference or context confirms this
- "contradicted" if either contradicts this
- "unverifiable" if neither addresses this

Return JSON: {{"label": "supported/contradicted/unverifiable", "reason": "brief explanation"}}"""

        try:
            response = self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "Judge claim against reference. Return JSON."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0,
                max_tokens=150,
                response_format={"type": "json_object"}
            )
            
            result = json.loads(response.choices[0].message.content)
            return {
                'claim': claim,
                'label': result.get('label', 'unverifiable'),
                'reason': result.get('reason', '')[:100]
            }
            
        except Exception as e:
            print(f"    Error judging: {e}")
            return {'claim': claim, 'label': 'unverifiable', 'reason': 'Error'}
    
    def evaluate_all(self):
        """–û—Ü–µ–Ω–∏–≤–∞–µ–º –≤—Å–µ –æ—Ç–≤–µ—Ç—ã"""
        
        print("="*80)
        print("–û–¶–ï–ù–ö–ê HALLUCINATION –ù–ê –†–ï–ê–õ–¨–ù–´–• –í–û–ü–†–û–°–ê–• –ò–ó –î–ê–¢–ê–°–ï–¢–ê")
        print("="*80)
        
        all_results = []
        
        for i, q_data in enumerate(self.results, 1):
            q_id = q_data['question_id']
            question = q_data['question_text']
            category = q_data['category']
            gold_answer = q_data['gold_answer']
            
            print(f"\nüìù –í–æ–ø—Ä–æ—Å {i}/{len(self.results)} (ID: {q_id})")
            print(f"   –í–æ–ø—Ä–æ—Å: {question[:60]}...")
            print(f"   –ö–∞—Ç–µ–≥–æ—Ä–∏—è: {category}")
            print(f"   –ó–æ–ª–æ—Ç–æ–π –æ—Ç–≤–µ—Ç (–∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞): {gold_answer[:80]}...")
            print("-"*60)
            
            q_metrics = {}
            
            # –û—Ü–µ–Ω–∏–≤–∞–µ–º –∫–∞–∂–¥—ã–π —Ä–µ–∂–∏–º
            for mode in ['base_llm', 'vector_rag', 'graph_rag', 'hybrid_ahs']:
                mode_data = q_data[mode]
                answer = mode_data['answer']
                context = mode_data.get('context_used', [])
                
                print(f"\n  üîç {mode}:")
                print(f"     –û—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏: {answer[:100]}...")
                
                # 1. –ò–∑–≤–ª–µ–∫–∞–µ–º claims
                claims = self.extract_claims(answer)
                print(f"     Claims –∏–∑–≤–ª–µ—á–µ–Ω–æ: {len(claims)}")
                
                # 2. –û—Ü–µ–Ω–∏–≤–∞–µ–º –∫–∞–∂–¥—ã–π claim
                judgments = []
                for claim_obj in claims:
                    claim_text = claim_obj.get('text', str(claim_obj))
                    judgment = self.judge_claim(
                        claim_text,
                        gold_answer,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ó–û–õ–û–¢–û–ô –æ—Ç–≤–µ—Ç –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞
                        context if isinstance(context, list) else [],
                        mode
                    )
                    judgments.append(judgment)
                
                # 3. –°—á–∏—Ç–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏
                total = len(judgments)
                if total > 0:
                    supported = sum(1 for j in judgments if j['label'] == 'supported')
                    contradicted = sum(1 for j in judgments if j['label'] == 'contradicted')
                    unverifiable = sum(1 for j in judgments if j['label'] == 'unverifiable')
                    
                    hr = (contradicted + unverifiable) / total
                    hr_contra = contradicted / total
                    hr_unver = unverifiable / total
                else:
                    supported = contradicted = unverifiable = 0
                    hr = hr_contra = hr_unver = 0
                
                print(f"     ‚úÖ Supported: {supported}/{total}")
                print(f"     ‚ùå Contradicted: {contradicted}/{total}")
                print(f"     ‚ùì Unverifiable: {unverifiable}/{total}")
                print(f"     üìä Hallucination Rate = {hr:.1%}")
                
                q_metrics[mode] = {
                    'total_claims': total,
                    'supported': supported,
                    'contradicted': contradicted,
                    'unverifiable': unverifiable,
                    'HR': hr,
                    'HR_contra': hr_contra,
                    'HR_unver': hr_unver
                }
            
            all_results.append({
                'question_id': q_id,
                'question': question,
                'category': category,
                'metrics': q_metrics
            })
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        with open('real_dataset_evaluation_results.json', 'w', encoding='utf-8') as f:
            json.dump(all_results, f, indent=2, ensure_ascii=False)
        
        # –ü–µ—á–∞—Ç–∞–µ–º –∏—Ç–æ–≥–∏
        self.print_summary(all_results)
    
    def print_summary(self, results):
        """–ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞"""
        
        print("\n" + "="*80)
        print("–ò–¢–û–ì–û–í–´–ï –ú–ï–¢–†–ò–ö–ò HALLUCINATION")
        print("="*80)
        
        # –°—Ä–µ–¥–Ω–∏–µ HR –ø–æ —Ä–µ–∂–∏–º–∞–º
        mode_hrs = {'base_llm': [], 'vector_rag': [], 'graph_rag': [], 'hybrid_ahs': []}
        
        for r in results:
            for mode in mode_hrs:
                mode_hrs[mode].append(r['metrics'][mode]['HR'])
        
        print("\nüìä –°—Ä–µ–¥–Ω–∏–π Hallucination Rate –ø–æ —Ä–µ–∂–∏–º–∞–º:")
        avg_hrs = {}
        for mode, hrs in mode_hrs.items():
            if hrs:
                avg = sum(hrs) / len(hrs)
                avg_hrs[mode] = avg
                print(f"  {mode:12} : {avg:.1%}")
        
        if avg_hrs:
            print("\nüèÜ –†–µ–π—Ç–∏–Ω–≥ —Ä–µ–∂–∏–º–æ–≤ (–º–µ–Ω—å—à–µ HR = –ª—É—á—à–µ):")
            sorted_modes = sorted(avg_hrs.items(), key=lambda x: x[1])
            for i, (mode, hr) in enumerate(sorted_modes, 1):
                emoji = "ü•á" if i == 1 else "ü•à" if i == 2 else "ü•â" if i == 3 else "4Ô∏è‚É£"
                print(f"  {emoji} {mode:12} : HR = {hr:.1%}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞—É—á–Ω—ã–µ –≥–∏–ø–æ—Ç–µ–∑—ã
        print("\n‚úÖ –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞—É—á–Ω—ã—Ö –≥–∏–ø–æ—Ç–µ–∑:")
        if avg_hrs:
            base_hr = avg_hrs.get('base_llm', 1.0)
            rag_hrs = [avg_hrs.get('vector_rag', 1.0), avg_hrs.get('graph_rag', 1.0), avg_hrs.get('hybrid_ahs', 1.0)]
            
            if base_hr > min(rag_hrs):
                reduction = (1 - min(rag_hrs)/base_hr) * 100
                print(f"  ‚úì RAG —Å–Ω–∏–∂–∞–µ—Ç hallucination –Ω–∞ {reduction:.0f}% –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å base_llm")
            else:
                print("  ‚úó RAG –ù–ï —Å–Ω–∏–∂–∞–µ—Ç hallucination")
            
            if 'hybrid_ahs' in avg_hrs and avg_hrs['hybrid_ahs'] == min(avg_hrs.values()):
                print("  ‚úì –ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–¥—Ö–æ–¥ (hybrid_ahs) –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –ª—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç")
            else:
                best = min(avg_hrs, key=avg_hrs.get) if avg_hrs else 'unknown'
                print(f"  ‚úó –õ—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç —É {best}, –Ω–µ —É hybrid_ahs")
        
        print("\nüíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ real_dataset_evaluation_results.json")
        print("\nüéØ –≠—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–ª—è –Ω–∞—É—á–Ω–æ–π —Å—Ç–∞—Ç—å–∏!")

def main():
    evaluator = RealDatasetEvaluator()
    evaluator.evaluate_all()

if __name__ == "__main__":
    main()