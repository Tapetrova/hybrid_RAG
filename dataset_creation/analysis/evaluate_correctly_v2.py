#!/usr/bin/env python3
"""
–ü–†–ê–í–ò–õ–¨–ù–ê–Ø –æ—Ü–µ–Ω–∫–∞ hallucination - –≤–µ—Ä—Å–∏—è 2:
- –ù–∞—Ö–æ–¥–∏–º –ù–ê–ò–ë–û–õ–ï–ï —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –∑–æ–ª–æ—Ç—ã–µ –æ—Ç–≤–µ—Ç—ã –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞
- –î–ª—è base_llm: –ø—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ –ó–û–õ–û–¢–û–ú–£ –û–¢–í–ï–¢–£ –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞
- –î–ª—è RAG —Ä–µ–∂–∏–º–æ–≤: –ø—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ –ó–û–õ–û–¢–û–ú–£ –û–¢–í–ï–¢–£ + retrieved –∫–æ–Ω—Ç–µ–∫—Å—Ç—É
"""

import json
import os
from typing import Dict, List
from openai import OpenAI
from dotenv import load_dotenv

load_dotenv()

class CorrectEvaluatorV2:
    def __init__(self):
        self.client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã 5 –≤–æ–ø—Ä–æ—Å–æ–≤
        with open('verify_4_modes_results.json', 'r') as f:
            self.results = json.load(f)
        
        # –ü–†–ê–í–ò–õ–¨–ù–û –∑–∞–≥—Ä—É–∂–∞–µ–º –∑–æ–ª–æ—Ç—ã–µ –æ—Ç–≤–µ—Ç—ã
        with open('../data/apqc_auto.json', 'r') as f:
            dataset = json.load(f)
        
        # –°–æ–∑–¥–∞—ë–º –º–∞–ø–∏–Ω–≥ –≤–æ–ø—Ä–æ—Å -> –∑–æ–ª–æ—Ç–æ–π –æ—Ç–≤–µ—Ç
        self.gold_answers = {}
        
        # –¢–æ—á–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã –∏–∑ —Ç–µ—Å—Ç–∞ –∏ –∏—Ö –ª—É—á—à–∏–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ
        test_to_dataset_mapping = {
            "Why does engine knock when cold?": {
                "keywords": ["knock", "cold", "engine", "start"],
                "category": "causal",
                "fallback_keywords": ["knock", "noise", "cold"]
            },
            "How to diagnose a misfire?": {
                "keywords": ["diagnose", "misfire", "cylinder"],
                "category": "diagnostic", 
                "fallback_keywords": ["diagnose", "troubleshoot", "engine"]
            },
            "What type of oil for 2020 Honda Civic?": {
                "keywords": ["oil", "honda", "civic", "type"],
                "category": "factual",
                "fallback_keywords": ["oil", "engine", "viscosity"]
            },
            "Drum brakes vs disc brakes performance?": {
                "keywords": ["drum", "disc", "brake", "performance"],
                "category": "comparative",
                "fallback_keywords": ["brake", "disc", "drum"]
            },
            "Why do brakes squeal after replacement?": {
                "keywords": ["brake", "squeal", "replacement", "noise"],
                "category": "causal",
                "fallback_keywords": ["squeal", "brake", "noise"]
            }
        }
        
        # –ü–æ–∏—Å–∫ –Ω–∞–∏–±–æ–ª–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤
        for test_q, search_params in test_to_dataset_mapping.items():
            best_match = None
            best_score = 0
            
            for item in dataset['questions']:
                q_lower = item['question'].lower()
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é
                if item['category'] != search_params['category']:
                    continue
                
                # –°—á–∏—Ç–∞–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
                score = 0
                for kw in search_params['keywords']:
                    if kw in q_lower:
                        score += 2  # –û—Å–Ω–æ–≤–Ω—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –≤–µ—Å—è—Ç –±–æ–ª—å—à–µ
                
                for kw in search_params['fallback_keywords']:
                    if kw in q_lower:
                        score += 1
                
                # –û–±–Ω–æ–≤–ª—è–µ–º –ª—É—á—à–µ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
                if score > best_score and len(item['answer']) > 200:
                    best_score = score
                    best_match = item
            
            # –ï—Å–ª–∏ –Ω–∞—à–ª–∏ —Ö–æ—Ä–æ—à–µ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ - –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ
            if best_match and best_score > 0:
                self.gold_answers[test_q] = best_match['answer']
                print(f"‚úì –ù–∞–π–¥–µ–Ω –∑–æ–ª–æ—Ç–æ–π –æ—Ç–≤–µ—Ç –¥–ª—è: {test_q[:50]}...")
                print(f"  –í–æ–ø—Ä–æ—Å –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ: {best_match['question'][:70]}...")
                print(f"  –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {best_score} –±–∞–ª–ª–æ–≤")
            else:
                # Fallback - –±–µ—Ä—ë–º –ø–µ—Ä–≤—ã–π –∏–∑ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
                for item in dataset['questions']:
                    if item['category'] == search_params['category'] and len(item['answer']) > 500:
                        self.gold_answers[test_q] = item['answer']
                        print(f"‚ö†Ô∏è –ò—Å–ø–æ–ª—å–∑—É—é –æ—Ç–≤–µ—Ç –∏–∑ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ '{search_params['category']}' –¥–ª—è: {test_q[:50]}...")
                        break
    
    def extract_claims(self, answer_text: str) -> List[Dict]:
        """–ò–∑–≤–ª–µ–∫–∞–µ–º —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è –∏–∑ –æ—Ç–≤–µ—Ç–∞"""
        
        prompt = f"""Extract key factual claims from this automotive answer.
Each claim should be a specific, verifiable statement.

Answer: {answer_text[:500]}

Return JSON array with max 5 claims:
{{"claims": [{{"text": "specific claim 1"}}, {{"text": "specific claim 2"}}]}}"""

        try:
            response = self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "Extract specific verifiable claims."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0,
                max_tokens=300,
                response_format={"type": "json_object"}
            )
            
            result = json.loads(response.choices[0].message.content)
            claims = result.get('claims', [])
            return claims[:5]  # –ú–∞–∫—Å–∏–º—É–º 5
            
        except Exception as e:
            print(f"    Error extracting claims: {e}")
            return []
    
    def judge_claim_correctly(self, claim: str, gold_answer: str, retrieved_context: List[str], mode: str) -> Dict:
        """–ü–†–ê–í–ò–õ–¨–ù–ê–Ø –æ—Ü–µ–Ω–∫–∞ claim"""
        
        # –î–ª—è base_llm –ø—Ä–æ–≤–µ—Ä—è–µ–º –¢–û–õ–¨–ö–û –ø–æ –∑–æ–ª–æ—Ç–æ–º—É –æ—Ç–≤–µ—Ç—É
        # –î–ª—è RAG —Ä–µ–∂–∏–º–æ–≤ –ø—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ –∑–æ–ª–æ—Ç–æ–º—É –æ—Ç–≤–µ—Ç—É + –∫–æ–Ω—Ç–µ–∫—Å—Ç—É
        
        if mode == 'base_llm':
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ–ª—å–∫–æ –ø–æ –∑–æ–ª–æ—Ç–æ–º—É –æ—Ç–≤–µ—Ç—É
            prompt = f"""Judge if this claim is supported by the reference answer.

Claim: {claim}

Reference Answer: {gold_answer[:800]}

Classify as:
- "supported" if the reference answer confirms this claim
- "contradicted" if the reference answer contradicts this claim  
- "unverifiable" if the reference answer doesn't address this claim

Return JSON: {{"label": "supported/contradicted/unverifiable", "reason": "brief explanation"}}"""
        else:
            # –î–ª—è RAG –ø—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ –∑–æ–ª–æ—Ç–æ–º—É –æ—Ç–≤–µ—Ç—É + retrieved –∫–æ–Ω—Ç–µ–∫—Å—Ç—É
            context_str = "\n".join(retrieved_context[:3]) if retrieved_context else "No context"
            
            prompt = f"""Judge if this claim is supported by the reference answer OR retrieved context.

Claim: {claim}

Reference Answer: {gold_answer[:500]}

Retrieved Context: {context_str[:500]}

Classify as:
- "supported" if either reference or context confirms this
- "contradicted" if either contradicts this
- "unverifiable" if neither addresses this

Return JSON: {{"label": "supported/contradicted/unverifiable", "reason": "brief explanation"}}"""

        try:
            response = self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "Judge claim against reference. Return JSON."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0,
                max_tokens=150,
                response_format={"type": "json_object"}
            )
            
            result = json.loads(response.choices[0].message.content)
            return {
                'claim': claim,
                'label': result.get('label', 'unverifiable'),
                'reason': result.get('reason', '')[:100]
            }
            
        except Exception as e:
            print(f"    Error judging: {e}")
            return {'claim': claim, 'label': 'unverifiable', 'reason': 'Error'}
    
    def evaluate_all(self):
        """–û—Ü–µ–Ω–∏–≤–∞–µ–º –≤—Å–µ –æ—Ç–≤–µ—Ç—ã –ü–†–ê–í–ò–õ–¨–ù–û"""
        
        print("="*80)
        print("–ü–†–ê–í–ò–õ–¨–ù–ê–Ø –û–¶–ï–ù–ö–ê HALLUCINATION (v2)")
        print("="*80)
        print(f"\n–ù–∞–π–¥–µ–Ω–æ –∑–æ–ª–æ—Ç—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤: {len(self.gold_answers)}/5\n")
        
        all_results = []
        
        for i, q_data in enumerate(self.results):
            question = q_data['question']['question']
            category = q_data['question']['category']
            
            print(f"\nüìù –í–æ–ø—Ä–æ—Å {i+1}: {question[:60]}...")
            print(f"   –ö–∞—Ç–µ–≥–æ—Ä–∏—è: {category}")
            
            # –ü–æ–ª—É—á–∞–µ–º –∑–æ–ª–æ—Ç–æ–π –æ—Ç–≤–µ—Ç
            gold = self.gold_answers.get(question, "")
            if gold:
                print(f"   ‚úÖ –ó–æ–ª–æ—Ç–æ–π –æ—Ç–≤–µ—Ç: {gold[:80]}...")
            else:
                print(f"   ‚ùå –ó–æ–ª–æ—Ç–æ–π –æ—Ç–≤–µ—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω!")
                gold = "Reference answer not available"
            
            print("-"*60)
            
            q_metrics = {}
            
            # –û—Ü–µ–Ω–∏–≤–∞–µ–º –∫–∞–∂–¥—ã–π —Ä–µ–∂–∏–º
            for mode in ['base_llm', 'vector_rag', 'graph_rag', 'hybrid_ahs']:
                answer = q_data[mode]['answer']
                context = q_data[mode].get('context_used', [])
                
                print(f"\n  üîç {mode}:")
                
                # 1. –ò–∑–≤–ª–µ–∫–∞–µ–º claims
                claims = self.extract_claims(answer)
                print(f"     Claims –∏–∑–≤–ª–µ—á–µ–Ω–æ: {len(claims)}")
                
                # 2. –û—Ü–µ–Ω–∏–≤–∞–µ–º –∫–∞–∂–¥—ã–π claim –ü–†–ê–í–ò–õ–¨–ù–û
                judgments = []
                for claim_obj in claims:
                    claim_text = claim_obj.get('text', str(claim_obj))
                    judgment = self.judge_claim_correctly(
                        claim_text,
                        gold,
                        context if isinstance(context, list) else [],
                        mode
                    )
                    judgments.append(judgment)
                
                # 3. –°—á–∏—Ç–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏
                total = len(judgments)
                if total > 0:
                    supported = sum(1 for j in judgments if j['label'] == 'supported')
                    contradicted = sum(1 for j in judgments if j['label'] == 'contradicted')
                    unverifiable = sum(1 for j in judgments if j['label'] == 'unverifiable')
                    
                    hr = (contradicted + unverifiable) / total
                    hr_contra = contradicted / total
                    hr_unver = unverifiable / total
                else:
                    supported = contradicted = unverifiable = 0
                    hr = hr_contra = hr_unver = 0
                
                print(f"     ‚úÖ Supported: {supported}/{total}")
                print(f"     ‚ùå Contradicted: {contradicted}/{total}")
                print(f"     ‚ùì Unverifiable: {unverifiable}/{total}")
                print(f"     üìä HR = {hr:.1%}")
                
                q_metrics[mode] = {
                    'total_claims': total,
                    'supported': supported,
                    'contradicted': contradicted,
                    'unverifiable': unverifiable,
                    'HR': hr,
                    'HR_contra': hr_contra,
                    'HR_unver': hr_unver
                }
            
            all_results.append({
                'question': question,
                'category': category,
                'has_gold': gold != "Reference answer not available",
                'metrics': q_metrics
            })
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        with open('correct_evaluation_v2_results.json', 'w', encoding='utf-8') as f:
            json.dump(all_results, f, indent=2, ensure_ascii=False)
        
        # –ü–µ—á–∞—Ç–∞–µ–º –∏—Ç–æ–≥–∏
        self.print_summary(all_results)
    
    def print_summary(self, results):
        """–ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞"""
        
        print("\n" + "="*80)
        print("–ò–¢–û–ì–û–í–´–ï –ú–ï–¢–†–ò–ö–ò HALLUCINATION (–ü–†–ê–í–ò–õ–¨–ù–ê–Ø –û–¶–ï–ù–ö–ê v2)")
        print("="*80)
        
        # –°—Ä–µ–¥–Ω–∏–µ HR –ø–æ —Ä–µ–∂–∏–º–∞–º
        mode_hrs = {'base_llm': [], 'vector_rag': [], 'graph_rag': [], 'hybrid_ahs': []}
        
        for r in results:
            if r['has_gold']:  # –£—á–∏—Ç—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ —Å –∑–æ–ª–æ—Ç—ã–º–∏ –æ—Ç–≤–µ—Ç–∞–º–∏
                for mode in mode_hrs:
                    mode_hrs[mode].append(r['metrics'][mode]['HR'])
        
        print("\nüìä –°—Ä–µ–¥–Ω–∏–π Hallucination Rate (—Å –∑–æ–ª–æ—Ç—ã–º–∏ –æ—Ç–≤–µ—Ç–∞–º–∏):")
        avg_hrs = {}
        for mode, hrs in mode_hrs.items():
            if hrs:
                avg = sum(hrs) / len(hrs)
                avg_hrs[mode] = avg
                print(f"  {mode:12} : {avg:.1%}")
        
        if avg_hrs:
            print("\nüèÜ –†–µ–π—Ç–∏–Ω–≥ (–º–µ–Ω—å—à–µ = –ª—É—á—à–µ):")
            sorted_modes = sorted(avg_hrs.items(), key=lambda x: x[1])
            for i, (mode, hr) in enumerate(sorted_modes, 1):
                emoji = "ü•á" if i == 1 else "ü•à" if i == 2 else "ü•â" if i == 3 else "4Ô∏è‚É£"
                print(f"  {emoji} {mode:12} : HR = {hr:.1%}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≥–∏–ø–æ—Ç–µ–∑—ã
        print("\n‚úÖ –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞—É—á–Ω—ã—Ö –≥–∏–ø–æ—Ç–µ–∑:")
        if avg_hrs:
            if avg_hrs['base_llm'] > min(avg_hrs['vector_rag'], avg_hrs['graph_rag'], avg_hrs['hybrid_ahs']):
                print("  ‚úì RAG —Å–Ω–∏–∂–∞–µ—Ç hallucination –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å base_llm")
            else:
                print("  ‚úó RAG –ù–ï —Å–Ω–∏–∂–∞–µ—Ç hallucination")
            
            if 'hybrid_ahs' in avg_hrs and avg_hrs['hybrid_ahs'] == min(avg_hrs.values()):
                print("  ‚úì –ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –ª—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç")
            else:
                best = min(avg_hrs, key=avg_hrs.get) if avg_hrs else 'unknown'
                print(f"  ‚úó –õ—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç —É {best}, –Ω–µ —É hybrid")
        
        print("\nüíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ correct_evaluation_v2_results.json")

def main():
    evaluator = CorrectEvaluatorV2()
    evaluator.evaluate_all()

if __name__ == "__main__":
    main()