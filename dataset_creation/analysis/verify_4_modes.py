#!/usr/bin/env python3
"""
–ü–†–û–í–ï–†–ö–ê –í–°–ï–• 4 –†–ï–ñ–ò–ú–û–í –ù–ê 5 –í–û–ü–†–û–°–ê–•
–° —Ä–µ–∞–ª—å–Ω—ã–º–∏ API –∏ —á—ë—Ç–∫–∏–º–∏ —Ä–∞–∑–ª–∏—á–∏—è–º–∏
"""

import json
import os
import time
import requests
from openai import OpenAI
from dotenv import load_dotenv

load_dotenv()

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
OPENAI_KEY = os.getenv('OPENAI_API_KEY')
TAVILY_KEY = "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"

# 5 —Ç–µ—Å—Ç–æ–≤—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤
TEST_QUESTIONS = [
    {
        "id": 1,
        "question": "Why does engine knock when cold?",
        "category": "causal"
    },
    {
        "id": 2,
        "question": "How to diagnose a misfire?",
        "category": "diagnostic"
    },
    {
        "id": 3,
        "question": "What type of oil for 2020 Honda Civic?",
        "category": "factual"
    },
    {
        "id": 4,
        "question": "Drum brakes vs disc brakes performance?",
        "category": "comparative"
    },
    {
        "id": 5,
        "question": "Why do brakes squeal after replacement?",
        "category": "causal"
    }
]

def test_base_llm(question: str) -> dict:
    """MODE 1: –ß–∏—Å—Ç—ã–π LLM –±–µ–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
    print("  ü§ñ base_llm: –ó–∞–ø—Ä–æ—Å –∫ OpenAI –ë–ï–ó –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞...")
    
    client = OpenAI(api_key=OPENAI_KEY)
    
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": "You are an automotive expert. Answer concisely."},
            {"role": "user", "content": question}
        ],
        temperature=0,
        max_tokens=150
    )
    
    return {
        "mode": "base_llm",
        "answer": response.choices[0].message.content,
        "context_used": None,
        "context_size": 0
    }

def get_tavily_context(query: str, search_type: str = "basic") -> list:
    """–ü–æ–ª—É—á–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç —á–µ—Ä–µ–∑ Tavily API"""
    
    url = "https://api.tavily.com/search"
    
    # –ú–æ–¥–∏—Ñ–∏—Ü–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞
    if search_type == "causal":
        query = f"why cause reason {query}"
    elif search_type == "diagnostic":
        query = f"diagnose troubleshoot symptoms {query}"
    
    payload = {
        "api_key": TAVILY_KEY,
        "query": f"automotive {query}",
        "search_depth": "basic",
        "max_results": 2,
        "include_domains": ["mechanics.stackexchange.com", "reddit.com/r/MechanicAdvice"]
    }
    
    try:
        response = requests.post(url, json=payload, timeout=10)
        if response.status_code == 200:
            results = response.json().get('results', [])
            return [r.get('content', '')[:300] for r in results]
    except Exception as e:
        print(f"    ‚ö†Ô∏è Tavily error: {e}")
    
    return []

def test_vector_rag(question: str) -> dict:
    """MODE 2: –í–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ + LLM"""
    print("  üìä vector_rag: –ü–æ–ª—É—á–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç —á–µ—Ä–µ–∑ Tavily (vector search)...")
    
    # –ü–æ–ª—É—á–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç —á–µ—Ä–µ–∑ –≤–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫
    contexts = get_tavily_context(question, "basic")
    
    if not contexts:
        print("    ‚ùå –ù–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –æ—Ç Tavily!")
        contexts = ["No additional context available."]
    
    context_str = "\n".join(contexts)
    print(f"    ‚úì –ü–æ–ª—É—á–µ–Ω–æ {len(contexts)} —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤, {len(context_str)} —Å–∏–º–≤–æ–ª–æ–≤")
    
    client = OpenAI(api_key=OPENAI_KEY)
    
    prompt = f"""Context from search:
{context_str}

Question: {question}
Answer based on context:"""
    
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": "Answer based on the provided context."},
            {"role": "user", "content": prompt}
        ],
        temperature=0,
        max_tokens=150
    )
    
    return {
        "mode": "vector_rag",
        "answer": response.choices[0].message.content,
        "context_used": contexts,
        "context_size": len(context_str)
    }

def test_graph_rag(question: str, category: str) -> dict:
    """MODE 3: –ì—Ä–∞—Ñ–æ–≤—ã–π –ø–æ–∏—Å–∫ —Å —É—á—ë—Ç–æ–º —Å–≤—è–∑–µ–π"""
    print("  üï∏Ô∏è graph_rag: –ü–æ–ª—É—á–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å —Ñ–æ–∫—É—Å–æ–º –Ω–∞ —Å–≤—è–∑–∏...")
    
    # –î–ª—è –≥—Ä–∞—Ñ–∞ –∏—â–µ–º –ø—Ä–∏—á–∏–Ω–Ω–æ-—Å–ª–µ–¥—Å—Ç–≤–µ–Ω–Ω—ã–µ —Å–≤—è–∑–∏
    contexts = get_tavily_context(question, "causal" if category in ["causal", "diagnostic"] else "basic")
    
    if not contexts:
        contexts = ["No graph relationships found."]
    
    context_str = "\n".join(contexts)
    print(f"    ‚úì –ì—Ä–∞—Ñ –∫–æ–Ω—Ç–µ–∫—Å—Ç: {len(context_str)} —Å–∏–º–≤–æ–ª–æ–≤")
    
    client = OpenAI(api_key=OPENAI_KEY)
    
    prompt = f"""Causal/relational context:
{context_str}

Question: {question}
Focus on cause-effect relationships:"""
    
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": "Focus on causal relationships and connections."},
            {"role": "user", "content": prompt}
        ],
        temperature=0,
        max_tokens=150
    )
    
    return {
        "mode": "graph_rag",
        "answer": response.choices[0].message.content,
        "context_used": contexts,
        "context_size": len(context_str)
    }

def test_hybrid_ahs(question: str, category: str) -> dict:
    """MODE 4: –ì–∏–±—Ä–∏–¥–Ω—ã–π —Ä–µ–∂–∏–º (vector + graph —Å –≤–µ—Å–∞–º–∏)"""
    print("  üîÑ hybrid_ahs: –ö–æ–º–±–∏–Ω–∏—Ä—É–µ–º vector –∏ graph –ø–æ–∏—Å–∫...")
    
    # –ü–æ–ª—É—á–∞–µ–º –æ–±–∞ —Ç–∏–ø–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
    vector_contexts = get_tavily_context(question, "basic")
    time.sleep(1)  # –ü–∞—É–∑–∞ –º–µ–∂–¥—É API –≤—ã–∑–æ–≤–∞–º–∏
    graph_contexts = get_tavily_context(question, "causal")
    
    # –ü—Ä–∏–º–µ–Ω—è–µ–º –≤–µ—Å–∞ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
    if category in ["causal", "diagnostic"]:
        # –ë–æ–ª—å—à–µ –≤–µ—Å–∞ –≥—Ä–∞—Ñ—É
        contexts = graph_contexts[:2] + vector_contexts[:1]
    else:
        # –ë–æ–ª—å—à–µ –≤–µ—Å–∞ –≤–µ–∫—Ç–æ—Ä—É
        contexts = vector_contexts[:2] + graph_contexts[:1]
    
    contexts = [c for c in contexts if c]  # –£–±–∏—Ä–∞–µ–º –ø—É—Å—Ç—ã–µ
    
    if not contexts:
        contexts = ["No hybrid context available."]
    
    context_str = "\n".join(contexts)
    print(f"    ‚úì –ì–∏–±—Ä–∏–¥–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç: {len(context_str)} —Å–∏–º–≤–æ–ª–æ–≤")
    
    client = OpenAI(api_key=OPENAI_KEY)
    
    prompt = f"""Combined vector and graph context:
{context_str}

Question: {question}
Comprehensive answer using all information:"""
    
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": "Provide comprehensive answer using all context."},
            {"role": "user", "content": prompt}
        ],
        temperature=0,
        max_tokens=150
    )
    
    return {
        "mode": "hybrid_ahs",
        "answer": response.choices[0].message.content,
        "context_used": contexts,
        "context_size": len(context_str)
    }

def main():
    print("="*80)
    print("–ü–†–û–í–ï–†–ö–ê –í–°–ï–• 4 –†–ï–ñ–ò–ú–û–í")
    print("="*80)
    
    results = []
    
    for i, q in enumerate(TEST_QUESTIONS, 1):
        print(f"\nüìù –í–æ–ø—Ä–æ—Å {i}/5: {q['question']}")
        print(f"   –ö–∞—Ç–µ–≥–æ—Ä–∏—è: {q['category']}")
        print("-"*60)
        
        # Test all 4 modes
        try:
            # Mode 1: base_llm
            result_base = test_base_llm(q['question'])
            print(f"    ‚úÖ base_llm: {result_base['answer'][:80]}...")
            
            # Mode 2: vector_rag
            result_vector = test_vector_rag(q['question'])
            print(f"    ‚úÖ vector_rag: {result_vector['answer'][:80]}...")
            
            # Mode 3: graph_rag
            result_graph = test_graph_rag(q['question'], q['category'])
            print(f"    ‚úÖ graph_rag: {result_graph['answer'][:80]}...")
            
            # Mode 4: hybrid_ahs
            result_hybrid = test_hybrid_ahs(q['question'], q['category'])
            print(f"    ‚úÖ hybrid_ahs: {result_hybrid['answer'][:80]}...")
            
            results.append({
                "question": q,
                "base_llm": result_base,
                "vector_rag": result_vector,
                "graph_rag": result_graph,
                "hybrid_ahs": result_hybrid
            })
            
        except Exception as e:
            print(f"    ‚ùå –û—à–∏–±–∫–∞: {e}")
        
        # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –≤–æ–ø—Ä–æ—Å–∞–º–∏
        if i < 5:
            time.sleep(2)
    
    # –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    print("\n" + "="*80)
    print("–ê–ù–ê–õ–ò–ó –†–ï–ó–£–õ–¨–¢–ê–¢–û–í")
    print("="*80)
    
    print("\nüìä –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞:")
    for mode in ["base_llm", "vector_rag", "graph_rag", "hybrid_ahs"]:
        sizes = [r[mode]["context_size"] for r in results]
        avg_size = sum(sizes) / len(sizes) if sizes else 0
        print(f"  {mode:12} : {avg_size:6.0f} —Å–∏–º–≤–æ–ª–æ–≤ –≤ —Å—Ä–µ–¥–Ω–µ–º")
    
    print("\n‚úÖ –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–±–æ—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏:")
    modes_status = {
        "base_llm": "‚úÖ –†–∞–±–æ—Ç–∞–µ—Ç (–±–µ–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞)",
        "vector_rag": "‚úÖ –†–∞–±–æ—Ç–∞–µ—Ç (—Å Tavily –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º)" if any(r["vector_rag"]["context_size"] > 0 for r in results) else "‚ùå –ù–µ —Ä–∞–±–æ—Ç–∞–µ—Ç",
        "graph_rag": "‚úÖ –†–∞–±–æ—Ç–∞–µ—Ç (—Å –∫–∞—É–∑–∞–ª—å–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º)" if any(r["graph_rag"]["context_size"] > 0 for r in results) else "‚ùå –ù–µ —Ä–∞–±–æ—Ç–∞–µ—Ç",
        "hybrid_ahs": "‚úÖ –†–∞–±–æ—Ç–∞–µ—Ç (–∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π)" if any(r["hybrid_ahs"]["context_size"] > 0 for r in results) else "‚ùå –ù–µ —Ä–∞–±–æ—Ç–∞–µ—Ç"
    }
    
    for mode, status in modes_status.items():
        print(f"  {mode:12} : {status}")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    with open("verify_4_modes_results.json", "w", encoding="utf-8") as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    print("\nüíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ verify_4_modes_results.json")
    
    print("\n" + "="*80)
    print("–ó–ê–ö–õ–Æ–ß–ï–ù–ò–ï")
    print("="*80)
    
    working_modes = sum(1 for status in modes_status.values() if "‚úÖ" in status)
    print(f"–†–∞–±–æ—Ç–∞—é—â–∏—Ö —Ä–µ–∂–∏–º–æ–≤: {working_modes}/4")
    
    if working_modes == 4:
        print("üéâ –í–°–ï 4 –†–ï–ñ–ò–ú–ê –†–ê–ë–û–¢–ê–Æ–¢!")
    else:
        print("‚ö†Ô∏è –ù–µ –≤—Å–µ —Ä–µ–∂–∏–º—ã —Ä–∞–±–æ—Ç–∞—é—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ")

if __name__ == "__main__":
    main()