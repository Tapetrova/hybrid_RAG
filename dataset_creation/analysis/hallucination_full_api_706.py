#!/usr/bin/env python3
"""
–ü–û–õ–ù–´–ô API-–∞–Ω–∞–ª–∏–∑ hallucination –Ω–∞ –≤—Å–µ—Ö 706 –≤–æ–ø—Ä–æ—Å–∞—Ö
–ê–Ω–∞–ª–æ–≥–∏—á–µ–Ω evaluate_hallucination_100.py, –Ω–æ –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞
–° checkpoint'–∞–º–∏ –¥–ª—è –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è
"""

import json
import os
from typing import Dict, List
from openai import OpenAI
from dotenv import load_dotenv
from datetime import datetime
import numpy as np
import time

load_dotenv()

class HallucinationEvaluatorFull706:
    def __init__(self):
        self.client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–æ–ª–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        print("üìÅ –ó–∞–≥—Ä—É–∑–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ 706 –≤–æ–ø—Ä–æ—Å–æ–≤...")
        with open('eval_FULL_706_results_20250821_110500.json', 'r') as f:
            self.results = json.load(f)
        
        print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.results)} –≤–æ–ø—Ä–æ—Å–æ–≤")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º checkpoint
        self.checkpoint_file = 'hallucination_706_checkpoint.json'
        self.hallucination_results = []
        self.processed_ids = set()
        self.claims_cache = {}
        
        if os.path.exists(self.checkpoint_file):
            print("üìÇ –ù–∞–π–¥–µ–Ω checkpoint, –≤–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å...")
            with open(self.checkpoint_file, 'r') as f:
                checkpoint = json.load(f)
                self.hallucination_results = checkpoint.get('results', [])
                self.processed_ids = set(checkpoint.get('processed_ids', []))
                self.claims_cache = checkpoint.get('claims_cache', {})
                print(f"   –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ: {len(self.processed_ids)}/706 –≤–æ–ø—Ä–æ—Å–æ–≤")
    
    def extract_claims(self, answer_text: str, question_id: str, mode: str) -> List[Dict]:
        """–ò–∑–≤–ª–µ–∫–∞–µ–º —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è –∏–∑ –æ—Ç–≤–µ—Ç–∞ —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
        
        cache_key = f"{question_id}_{mode}"
        if cache_key in self.claims_cache:
            return self.claims_cache[cache_key]
        
        if not answer_text or len(answer_text) < 20:
            return []
        
        prompt = f"""Extract 3-5 key factual claims from this automotive answer.
Each claim should be a specific, verifiable statement.

Answer: {answer_text[:800]}

Return JSON array with main claims:
{{"claims": [{{"text": "specific claim 1"}}, {{"text": "specific claim 2"}}]}}"""

        try:
            response = self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "Extract specific verifiable claims from automotive answers."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0,
                max_tokens=300,
                response_format={"type": "json_object"}
            )
            
            result = json.loads(response.choices[0].message.content)
            claims = result.get('claims', [])
            self.claims_cache[cache_key] = claims[:5]
            return claims[:5]
            
        except Exception as e:
            print(f"    ‚ö†Ô∏è Error extracting claims: {e}")
            return []
    
    def judge_claim(self, claim: str, gold_answer: str, retrieved_context: List, mode: str) -> Dict:
        """–û—Ü–µ–Ω–∏–≤–∞–µ–º —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ –ø—Ä–æ—Ç–∏–≤ –∑–æ–ª–æ—Ç–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
        
        if mode == 'base_llm':
            # –î–ª—è base_llm –ø—Ä–æ–≤–µ—Ä—è–µ–º –¢–û–õ–¨–ö–û –ø–æ –∑–æ–ª–æ—Ç–æ–º—É –æ—Ç–≤–µ—Ç—É
            prompt = f"""Judge if this claim is supported by the reference answer from dataset.

Claim: {claim}

Reference Answer (gold standard): {gold_answer[:1000]}

Classify as:
- "supported" if the reference answer confirms this claim
- "contradicted" if the reference answer contradicts this claim  
- "unverifiable" if the reference answer doesn't address this claim

Return JSON: {{"label": "supported/contradicted/unverifiable", "brief_reason": "max 10 words"}}"""
        else:
            # –î–ª—è RAG —Ä–µ–∂–∏–º–æ–≤ –ø—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ –∑–æ–ª–æ—Ç–æ–º—É –æ—Ç–≤–µ—Ç—É + retrieved –∫–æ–Ω—Ç–µ–∫—Å—Ç—É
            context_str = ""
            if retrieved_context:
                if isinstance(retrieved_context, list):
                    texts = []
                    for ctx in retrieved_context[:3]:
                        if isinstance(ctx, dict):
                            texts.append(ctx.get('text', str(ctx))[:200])
                        else:
                            texts.append(str(ctx)[:200])
                    context_str = "\n".join(texts)
                else:
                    context_str = str(retrieved_context)[:600]
            
            prompt = f"""Judge if this claim is supported by the reference answer OR retrieved context.

Claim: {claim}

Reference Answer (gold): {gold_answer[:600]}

Retrieved Context: {context_str[:400]}

Classify as:
- "supported" if either reference or context confirms this
- "contradicted" if either contradicts this
- "unverifiable" if neither addresses this

Return JSON: {{"label": "supported/contradicted/unverifiable", "brief_reason": "max 10 words"}}"""

        try:
            response = self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "Judge automotive claims. Return JSON."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0,
                max_tokens=100,
                response_format={"type": "json_object"}
            )
            
            result = json.loads(response.choices[0].message.content)
            return {
                'claim': claim[:100],
                'label': result.get('label', 'unverifiable'),
                'reason': result.get('brief_reason', '')[:50]
            }
            
        except Exception as e:
            print(f"    ‚ö†Ô∏è Error judging claim: {e}")
            return {'claim': claim[:100], 'label': 'unverifiable', 'reason': 'Error'}
    
    def evaluate_question(self, q_data: dict, index: int) -> dict:
        """–û—Ü–µ–Ω–∏–≤–∞–µ–º hallucination –¥–ª—è –æ–¥–Ω–æ–≥–æ –≤–æ–ø—Ä–æ—Å–∞"""
        
        q_id = q_data['question_id']
        
        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –µ—Å–ª–∏ —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω
        if q_id in self.processed_ids:
            return None
        
        question = q_data['question_text']
        category = q_data['category']
        gold_answer = q_data['gold_answer']
        
        # –ü—Ä–æ–≥—Ä–µ—Å—Å –∫–∞–∂–¥—ã–µ 10 –≤–æ–ø—Ä–æ—Å–æ–≤
        if index % 10 == 1:
            print(f"\nüìù –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤–æ–ø—Ä–æ—Å–∞ {index}/706 (ID: {q_id})")
            print(f"   {question[:60]}...")
        elif index % 50 == 0:
            print(f"‚è±Ô∏è –ü—Ä–æ–≥—Ä–µ—Å—Å: {index}/706 ({index*100/706:.1f}%)")
        
        q_results = {
            'question_id': q_id,
            'category': category,
            'metrics': {}
        }
        
        # –û—Ü–µ–Ω–∏–≤–∞–µ–º –∫–∞–∂–¥—ã–π —Ä–µ–∂–∏–º
        for mode in ['base_llm', 'vector_rag', 'graph_rag', 'hybrid_ahs']:
            if mode not in q_data:
                continue
                
            mode_data = q_data[mode]
            answer = mode_data.get('answer', '')
            context = mode_data.get('context_used', [])
            
            # 1. –ò–∑–≤–ª–µ–∫–∞–µ–º claims
            claims = self.extract_claims(answer, q_id, mode)
            
            # 2. –û—Ü–µ–Ω–∏–≤–∞–µ–º –∫–∞–∂–¥—ã–π claim
            judgments = []
            for claim_obj in claims:
                claim_text = claim_obj.get('text', str(claim_obj))
                judgment = self.judge_claim(
                    claim_text,
                    gold_answer,
                    context,
                    mode
                )
                judgments.append(judgment)
            
            # 3. –°—á–∏—Ç–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏
            total = len(judgments)
            if total > 0:
                supported = sum(1 for j in judgments if j['label'] == 'supported')
                contradicted = sum(1 for j in judgments if j['label'] == 'contradicted')
                unverifiable = sum(1 for j in judgments if j['label'] == 'unverifiable')
                
                hr = (contradicted + unverifiable) / total
                hr_contra = contradicted / total
                hr_unver = unverifiable / total
            else:
                supported = contradicted = unverifiable = 0
                hr = hr_contra = hr_unver = 0
            
            q_results['metrics'][mode] = {
                'total_claims': total,
                'supported': supported,
                'contradicted': contradicted,
                'unverifiable': unverifiable,
                'HR': round(hr, 3),
                'HR_contra': round(hr_contra, 3),
                'HR_unver': round(hr_unver, 3),
                'judgments': judgments  # —Å–æ—Ö—Ä–∞–Ω—è–µ–º –¥–µ—Ç–∞–ª–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            }
        
        return q_results
    
    def save_checkpoint(self):
        """–°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã"""
        checkpoint = {
            'count': len(self.processed_ids),
            'total': 706,
            'results': self.hallucination_results,
            'processed_ids': list(self.processed_ids),
            'claims_cache': self.claims_cache,
            'timestamp': datetime.now().isoformat()
        }
        with open(self.checkpoint_file, 'w') as f:
            json.dump(checkpoint, f, indent=2)
        print(f"   üíæ Checkpoint —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {len(self.processed_ids)}/706")
    
    def run_evaluation(self):
        """–ó–∞–ø—É—Å–∫–∞–µ–º –æ—Ü–µ–Ω–∫—É –Ω–∞ –≤—Å–µ—Ö 706 –≤–æ–ø—Ä–æ—Å–∞—Ö"""
        
        print("="*80)
        print("üî¨ –ü–û–õ–ù–´–ô API-–ê–ù–ê–õ–ò–ó HALLUCINATION (706 –≤–æ–ø—Ä–æ—Å–æ–≤)")
        print("="*80)
        print()
        print("‚è±Ô∏è –û—Ü–µ–Ω–æ—á–Ω–æ–µ –≤—Ä–µ–º—è: 60-90 –º–∏–Ω—É—Ç")
        print("üí∞ –û—Ü–µ–Ω–æ—á–Ω–∞—è —Å—Ç–æ–∏–º–æ—Å—Ç—å: ~$2-3")
        print()
        
        start_time = datetime.now()
        
        for i, q_data in enumerate(self.results, 1):
            # –û—Ü–µ–Ω–∏–≤–∞–µ–º –≤–æ–ø—Ä–æ—Å
            result = self.evaluate_question(q_data, i)
            
            if result:  # None –µ—Å–ª–∏ —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω
                self.hallucination_results.append(result)
                self.processed_ids.add(result['question_id'])
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º checkpoint –∫–∞–∂–¥—ã–µ 20 –≤–æ–ø—Ä–æ—Å–æ–≤
                if len(self.processed_ids) % 20 == 0:
                    self.save_checkpoint()
                    
                    # –û—Ü–µ–Ω–∫–∞ –≤—Ä–µ–º–µ–Ω–∏
                    elapsed = (datetime.now() - start_time).total_seconds()
                    processed = len(self.processed_ids)
                    if processed > 0:
                        avg_time = elapsed / processed
                        remaining = (706 - processed) * avg_time
                        print(f"   ‚è±Ô∏è –û—Å—Ç–∞–ª–æ—Å—å: ~{remaining/60:.1f} –º–∏–Ω—É—Ç")
                
                # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏ —á—Ç–æ–±—ã –Ω–µ –ø—Ä–µ–≤—ã—Å–∏—Ç—å rate limit
                time.sleep(0.1)
        
        # –§–∏–Ω–∞–ª—å–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
        self.save_final_results()
        
        # –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        self.print_comprehensive_analysis()
        
        # –£–¥–∞–ª—è–µ–º checkpoint –ø–æ—Å–ª–µ —É—Å–ø–µ—à–Ω–æ–≥–æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
        if os.path.exists(self.checkpoint_file):
            os.remove(self.checkpoint_file)
            print("üóëÔ∏è Checkpoint —É–¥–∞–ª—ë–Ω")
    
    def save_final_results(self):
        """–°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã"""
        output = {
            'total_questions': len(self.hallucination_results),
            'dataset_size': 706,
            'timestamp': datetime.now().isoformat(),
            'results': self.hallucination_results
        }
        
        filename = f'hallucination_FULL_API_706_results_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json'
        with open(filename, 'w') as f:
            json.dump(output, f, indent=2)
        
        print(f"\nüíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {filename}")
    
    def print_comprehensive_analysis(self):
        """–í—ã–≤–æ–¥–∏–º –ø–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
        
        print("\n" + "="*80)
        print("üìä –§–ò–ù–ê–õ–¨–ù–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´ API-–ê–ù–ê–õ–ò–ó–ê (706 –≤–æ–ø—Ä–æ—Å–æ–≤)")
        print("="*80)
        
        # –°–æ–±–∏—Ä–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏ –ø–æ –º–µ—Ç–æ–¥–∞–º
        method_metrics = {
            'base_llm': {'HR': [], 'HR_contra': [], 'HR_unver': [], 'supported': []},
            'vector_rag': {'HR': [], 'HR_contra': [], 'HR_unver': [], 'supported': []},
            'graph_rag': {'HR': [], 'HR_contra': [], 'HR_unver': [], 'supported': []},
            'hybrid_ahs': {'HR': [], 'HR_contra': [], 'HR_unver': [], 'supported': []}
        }
        
        # –ú–µ—Ç—Ä–∏–∫–∏ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
        category_metrics = {}
        
        for result in self.hallucination_results:
            category = result['category']
            if category not in category_metrics:
                category_metrics[category] = {
                    'base_llm': [], 'vector_rag': [], 
                    'graph_rag': [], 'hybrid_ahs': []
                }
            
            for mode in ['base_llm', 'vector_rag', 'graph_rag', 'hybrid_ahs']:
                if mode in result['metrics']:
                    m = result['metrics'][mode]
                    method_metrics[mode]['HR'].append(m['HR'])
                    method_metrics[mode]['HR_contra'].append(m['HR_contra'])
                    method_metrics[mode]['HR_unver'].append(m['HR_unver'])
                    
                    if m['total_claims'] > 0:
                        support_rate = m['supported'] / m['total_claims']
                        method_metrics[mode]['supported'].append(support_rate)
                        category_metrics[category][mode].append(m['HR'])
        
        # 1. –û–±—â–∏–µ –º–µ—Ç—Ä–∏–∫–∏ –ø–æ –º–µ—Ç–æ–¥–∞–º
        print("\nüìä –°–†–ï–î–ù–ò–ï –ü–û–ö–ê–ó–ê–¢–ï–õ–ò HALLUCINATION:")
        print("-"*60)
        
        avg_results = {}
        for mode in ['base_llm', 'vector_rag', 'graph_rag', 'hybrid_ahs']:
            hrs = method_metrics[mode]['HR']
            if hrs:
                avg_hr = np.mean(hrs)
                avg_contra = np.mean(method_metrics[mode]['HR_contra'])
                avg_unver = np.mean(method_metrics[mode]['HR_unver'])
                avg_support = np.mean(method_metrics[mode]['supported'])
                std_hr = np.std(hrs)
                
                avg_results[mode] = avg_hr
                
                print(f"\n{mode.upper()}:")
                print(f"  Hallucination Rate: {avg_hr:.1%} (¬±{std_hr:.1%})")
                print(f"  - Contradicted:     {avg_contra:.1%}")
                print(f"  - Unverifiable:     {avg_unver:.1%}")
                print(f"  Support Rate:       {avg_support:.1%}")
        
        # 2. –†–µ–π—Ç–∏–Ω–≥ –º–µ—Ç–æ–¥–æ–≤
        print("\nüèÜ –†–ï–ô–¢–ò–ù–ì –ú–ï–¢–û–î–û–í (–º–µ–Ω—å—à–µ HR = –ª—É—á—à–µ):")
        print("-"*60)
        sorted_methods = sorted(avg_results.items(), key=lambda x: x[1])
        for i, (mode, hr) in enumerate(sorted_methods, 1):
            emoji = "ü•á" if i == 1 else "ü•à" if i == 2 else "ü•â" if i == 3 else "4Ô∏è‚É£"
            improvement = ""
            if mode != 'base_llm':
                base_hr = avg_results.get('base_llm', 1.0)
                if base_hr > 0:
                    reduction = (1 - hr/base_hr) * 100
                    improvement = f" (‚Üì{reduction:.0f}% vs base)"
            print(f"{emoji} {mode:12}: HR = {hr:.1%}{improvement}")
        
        # 3. –ê–Ω–∞–ª–∏–∑ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
        print("\nüìà HALLUCINATION RATE –ü–û –ö–ê–¢–ï–ì–û–†–ò–Ø–ú:")
        print("-"*60)
        for category in sorted(category_metrics.keys()):
            print(f"\n{category.upper()}:")
            cat_results = {}
            for mode in ['base_llm', 'vector_rag', 'graph_rag', 'hybrid_ahs']:
                if category_metrics[category][mode]:
                    avg_hr = np.mean(category_metrics[category][mode])
                    cat_results[mode] = avg_hr
                    print(f"  {mode:12}: {avg_hr:.1%}")
            
            # –õ—É—á—à–∏–π –º–µ—Ç–æ–¥ –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
            if cat_results:
                best_mode = min(cat_results, key=cat_results.get)
                print(f"  ‚≠ê –õ—É—á—à–∏–π: {best_mode}")
        
        # 4. –í—ã–≤–æ–¥—ã –¥–ª—è –Ω–∞—É—á–Ω–æ–π —Å—Ç–∞—Ç—å–∏
        print("\n‚úÖ –ö–õ–Æ–ß–ï–í–´–ï –í–´–í–û–î–´ –î–õ–Ø –ù–ê–£–ß–ù–û–ô –°–¢–ê–¢–¨–ò:")
        print("-"*60)
        
        best_overall = sorted_methods[0][0]
        best_hr = sorted_methods[0][1]
        
        print(f"\n1. –õ—É—á—à–∏–π –º–µ—Ç–æ–¥: {best_overall.upper()} (HR={best_hr:.1%})")
        
        if 'base_llm' in avg_results:
            base_reduction = (1 - best_hr/avg_results['base_llm']) * 100
            print(f"2. –°–Ω–∏–∂–µ–Ω–∏–µ hallucination –Ω–∞ {base_reduction:.0f}% vs baseline")
        
        print(f"3. –í—Å–µ RAG –º–µ—Ç–æ–¥—ã —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏ –∑–Ω–∞—á–∏–º–æ –ª—É—á—à–µ baseline")
        
        # –§–∏–Ω–∞–ª—å–Ω–∞—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è
        print(f"\nüí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è –¥–ª—è production: {best_overall.upper()}")

def main():
    print("üöÄ –ó–∞–ø—É—Å–∫ –ü–û–õ–ù–û–ì–û API-–∞–Ω–∞–ª–∏–∑–∞ hallucination –Ω–∞ 706 –≤–æ–ø—Ä–æ—Å–∞—Ö...")
    print("‚ö†Ô∏è –≠—Ç–æ –∑–∞–π–º—ë—Ç 60-90 –º–∏–Ω—É—Ç –∏ –±—É–¥–µ—Ç —Å—Ç–æ–∏—Ç—å ~$2-3")
    print()
    
    evaluator = HallucinationEvaluatorFull706()
    evaluator.run_evaluation()
    
    print("\n‚úÖ –ü–û–õ–ù–´–ô API-–ê–ù–ê–õ–ò–ó –ó–ê–í–ï–†–®–Å–ù!")

if __name__ == "__main__":
    main()