#!/usr/bin/env python3
"""
–£–ø—Ä–æ—â—ë–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –¥–ª—è –æ—Ü–µ–Ω–∫–∏ 100 –≤–æ–ø—Ä–æ—Å–æ–≤
"""

import json
import os
import time
import requests
from openai import OpenAI
from dotenv import load_dotenv
from datetime import datetime

load_dotenv()

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
OPENAI_KEY = os.getenv('OPENAI_API_KEY')
TAVILY_KEY = "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"

print("üöÄ –ó–∞–ø—É—Å–∫ –æ—Ü–µ–Ω–∫–∏ –ø–µ—Ä–≤—ã—Ö 100 –≤–æ–ø—Ä–æ—Å–æ–≤ –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞...")
print(f"‚è∞ –ù–∞—á–∞–ª–æ: {datetime.now().strftime('%H:%M:%S')}")

# –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç
with open('../data/apqc_auto.json', 'r') as f:
    dataset = json.load(f)

print(f"üìö –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(dataset['questions'])} –≤–æ–ø—Ä–æ—Å–æ–≤")

client = OpenAI(api_key=OPENAI_KEY)
results = []

def get_tavily_context(query):
    """–ü—Ä–æ—Å—Ç–æ–π –∑–∞–ø—Ä–æ—Å –∫ Tavily"""
    try:
        response = requests.post(
            "https://api.tavily.com/search",
            json={
                "api_key": TAVILY_KEY,
                "query": f"automotive {query[:100]}",
                "search_depth": "basic",
                "max_results": 2
            },
            timeout=10
        )
        if response.status_code == 200:
            data = response.json()
            return [r.get('content', '')[:200] for r in data.get('results', [])]
    except:
        pass
    return []

# –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 100 –≤–æ–ø—Ä–æ—Å–æ–≤
for i, q in enumerate(dataset['questions'][:100], 1):
    print(f"\nüìù –í–æ–ø—Ä–æ—Å {i}/100 (ID: {q['id']})")
    print(f"   {q['question'][:60]}...")
    
    result = {
        "id": q['id'],
        "question": q['question'],
        "category": q['category'],
        "gold_answer": q['answer']
    }
    
    try:
        # 1. Base LLM
        print("   ü§ñ base_llm...", end="")
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "You are an automotive expert. Answer concisely."},
                {"role": "user", "content": q['question']}
            ],
            temperature=0,
            max_tokens=150
        )
        result['base_llm'] = {
            "answer": response.choices[0].message.content,
            "context_size": 0
        }
        print(" ‚úì")
        
        # 2. Vector RAG
        print("   üìä vector_rag...", end="")
        contexts = get_tavily_context(q['question'])
        context_str = "\n".join(contexts) if contexts else "No context"
        
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "Answer based on context."},
                {"role": "user", "content": f"Context: {context_str}\n\nQuestion: {q['question']}"}
            ],
            temperature=0,
            max_tokens=150
        )
        result['vector_rag'] = {
            "answer": response.choices[0].message.content,
            "context_size": len(context_str)
        }
        print(" ‚úì")
        
        # 3. Graph RAG
        print("   üï∏Ô∏è graph_rag...", end="")
        causal_query = f"why cause {q['question']}" if q['category'] in ['causal', 'diagnostic'] else q['question']
        contexts = get_tavily_context(causal_query)
        context_str = "\n".join(contexts) if contexts else "No context"
        
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "Focus on causal relationships."},
                {"role": "user", "content": f"Context: {context_str}\n\nQuestion: {q['question']}"}
            ],
            temperature=0,
            max_tokens=150
        )
        result['graph_rag'] = {
            "answer": response.choices[0].message.content,
            "context_size": len(context_str)
        }
        print(" ‚úì")
        
        # 4. Hybrid
        print("   üîÑ hybrid...", end="")
        contexts1 = get_tavily_context(q['question'])
        time.sleep(0.5)
        contexts2 = get_tavily_context(f"why {q['question']}")
        all_contexts = contexts1[:1] + contexts2[:1]
        context_str = "\n".join(all_contexts) if all_contexts else "No context"
        
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "Comprehensive answer."},
                {"role": "user", "content": f"Context: {context_str}\n\nQuestion: {q['question']}"}
            ],
            temperature=0,
            max_tokens=150
        )
        result['hybrid_ahs'] = {
            "answer": response.choices[0].message.content,
            "context_size": len(context_str)
        }
        print(" ‚úì")
        
        results.append(result)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∂–¥—ã–µ 10 –≤–æ–ø—Ä–æ—Å–æ–≤
        if i % 10 == 0:
            with open('eval_100_checkpoint.json', 'w') as f:
                json.dump({"count": i, "results": results}, f, indent=2, ensure_ascii=False)
            print(f"\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {i} –≤–æ–ø—Ä–æ—Å–æ–≤")
            print(f"‚è±Ô∏è –í—Ä–µ–º—è: {datetime.now().strftime('%H:%M:%S')}")
        
        # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –≤–æ–ø—Ä–æ—Å–∞–º–∏
        time.sleep(1)
        
    except Exception as e:
        print(f" ‚ùå –û—à–∏–±–∫–∞: {e}")
        continue

# –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
output_file = f"eval_100_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
with open(output_file, 'w') as f:
    json.dump(results, f, indent=2, ensure_ascii=False)

print("\n" + "="*60)
print("‚úÖ –ì–û–¢–û–í–û!")
print(f"üìÅ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {output_file}")
print(f"üìä –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –≤–æ–ø—Ä–æ—Å–æ–≤: {len(results)}")
print(f"‚è∞ –ö–æ–Ω–µ—Ü: {datetime.now().strftime('%H:%M:%S')}")